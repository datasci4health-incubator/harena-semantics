{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir funções utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(bert_model, tokenizer, text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "   \n",
    "    hidden_states = get_hidden_states(bert_model, tokenizer, text)\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar os embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### com Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "\n",
    "# MODEL = 'NLI'\n",
    "# MODEL_PATH = '../../models/' + MODEL + '/pytorch_model.bin'\n",
    "\n",
    "# md = torch.load(MODEL_PATH ,map_location='cpu')\n",
    "\n",
    "# embeds = []\n",
    "# for k in md:\n",
    "#     if (k == 'embeddings.word_embeddings.weight'):\n",
    "#         embeds = md[k]\n",
    "        \n",
    "# X = np.array(embeds)\n",
    "\n",
    "# words = []\n",
    "# with open('../../models/' + MODEL + '/vocab.txt', \"r\") as f:\n",
    "#     words = f.readlines()\n",
    "    \n",
    "# X_reduzido = X[12000:28125]\n",
    "# words_reduzido = words[12000:28125]\n",
    "# # X_reduzido = X\n",
    "# # words_reduzido = words\n",
    "\n",
    "# print(\"Tamanho: \",len(X_reduzido))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### com transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at fagner/envoy were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at fagner/envoy and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho:  28996\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "MODEL_PATH = 'fagner/envoy'\n",
    "# MODEL_PATH = 'bert-base-cased'\n",
    "# MODEL_PATH = 'dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False)\n",
    "model = BertModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "embeds = model.state_dict()['embeddings.word_embeddings.weight'] \n",
    "\n",
    "words = list(tokenizer.vocab.keys())\n",
    "\n",
    "X_reduzido = embeds\n",
    "words_reduzido = words\n",
    "\n",
    "# X_reduzido = embeds[12000:28125]\n",
    "# words_reduzido = words[12000:28125]\n",
    "\n",
    "print('Tamanho: ',len(words_reduzido))\n",
    "\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções pro plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# import os\n",
    "\n",
    "IMG_PATH = 'imgs/'\n",
    "\n",
    "# if not os.path.exists(IMG_PATH):\n",
    "#     os.mkdir(IMG_PATH)\n",
    "\n",
    "# def plot_clustering(X_red, labels, title=None):\n",
    "#     plt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n",
    "#     x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n",
    "#     X_red = (X_red - x_min) / (x_max - x_min)\n",
    "\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     print(X_red)\n",
    "#     for i in range(X_red.shape[0]):\n",
    "# #         print(X_red[i, 0])\n",
    "# #         print(X_red[i, 1])\n",
    "#         plt.text(X_red[i, 0], X_red[i, 1], str(words_reduzido[i]),\n",
    "#                  color=plt.cm.nipy_spectral(labels[i] / 10.),\n",
    "#                  fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     if title is not None:\n",
    "#         plt.title(title, size=17)\n",
    "#     plt.axis('off')\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#     plt.savefig(IMG_PATH + title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import dendrogram\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_dendrogram(model, **kwargs):\n",
    "#     # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "#     # create the counts of samples under each node\n",
    "#     counts = np.zeros(model.children_.shape[0])\n",
    "#     n_samples = len(model.labels_)\n",
    "#     for i, merge in enumerate(model.children_):\n",
    "#         current_count = 0\n",
    "#         for child_idx in merge:\n",
    "#             if child_idx < n_samples:\n",
    "#                 current_count += 1  # leaf node\n",
    "#             else:\n",
    "#                 current_count += counts[child_idx - n_samples]\n",
    "#         counts[i] = current_count\n",
    "\n",
    "#     linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "#                                       counts]).astype(float)\n",
    "\n",
    "#     # Plot the corresponding dendrogram\n",
    "#     dendrogram(linkage_matrix, **kwargs)\n",
    "#     IMG_NAME = MODEL_PATH.replace('/', \"-\").replace('.', \"-\")\n",
    "\n",
    "#     plt.savefig(IMG_PATH + IMG_NAME + '-dendrograma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import manifold\n",
    "\n",
    "clustering = AgglomerativeClustering(linkage='ward', distance_threshold=100, n_clusters=None)\n",
    "t0 = time()\n",
    "clustering_model = clustering.fit(X_reduzido)\n",
    "print(model)\n",
    "print(\"%s :\\t%.2fs\" % ('Duração: ', time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "X_2d = manifold.SpectralEmbedding(n_components=2).fit_transform(X_reduzido)\n",
    " \n",
    "x = X_2d[:,0]\n",
    "y = X_2d[:,1]\n",
    "\n",
    "\n",
    "plt.scatter(x, y, color='b', s=1)\n",
    "\n",
    "# for i, sentence in enumerate(sentences):\n",
    "#     # plt.annotate(clustering_model.labels_[i], (x[i], y[i]), color='gray', fontsize=18)\n",
    "#     plt.annotate(clustering_model.labels_[i], (x[i], y[i]), color=plt.cm.nipy_spectral(clustering_model.labels_[i] / 10.), fontsize=5)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "# plt.savefig(OUTPUT_PATH + str(n_clusters) + 'topics/clusters-axis-off-'+str(MODEL).replace('/','').replace('.',''))\n",
    "plt.savefig(IMG_PATH + MODEL_PATH.replace('/', \"-\").replace('.', \"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(IMG_PATH + MODEL_PATH.replace('/', \"-\").replace('.', \"-\"))\n",
    "# from sklearn import manifold\n",
    "\n",
    "# # X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X_reduzido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMG_NAME = MODEL_PATH.replace('/', \"-\").replace('.', \"-\")\n",
    "# print(IMG_NAME)\n",
    "# plot_clustering(X_red, clustering.labels_, IMG_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_dendrogram(model, truncate_mode='level', p=2)\n",
    "# plt.xlabel(\"In parenthesis, the number of tokens within the cluster.\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(clustering_model.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
