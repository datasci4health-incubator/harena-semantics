{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização hieráquica no BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pegar os embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489e1e90197242cca8b1ccfe319d44f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.position_ids \t torch.Size([1, 512])\n",
      "embeddings.word_embeddings.weight \t torch.Size([28996, 768])\n",
      "embeddings.position_embeddings.weight \t torch.Size([512, 768])\n",
      "embeddings.token_type_embeddings.weight \t torch.Size([2, 768])\n",
      "embeddings.LayerNorm.weight \t torch.Size([768])\n",
      "embeddings.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight \t torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias \t torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight \t torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias \t torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight \t torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias \t torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight \t torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias \t torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight \t torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias \t torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight \t torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias \t torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight \t torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias \t torch.Size([768])\n",
      "pooler.dense.weight \t torch.Size([768, 768])\n",
      "pooler.dense.bias \t torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "MODEL_PATH = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False)\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(md[k]))\n",
    "# list(tokenizer.vocab.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (1.9.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch) (3.10.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# MODEL_PATH = 'fagner/envoy'\n",
    "\n",
    "# md = \n",
    "\n",
    "md = torch.load('../../models/NLI/pytorch_model.bin',map_location='cpu')\n",
    "# md = torch.load('../models/bert/pytorch_model.bin',map_location='cpu')\n",
    "# md = torch.load('../harena-asm/models/biobert/pytorch_model.bin',map_location='cpu')\n",
    "# md = torch.load('../models/biobert/pytorch_model.bin',map_location='cpu')\n",
    "# md = torch.load('../harena-asm/models/bert-large-cased/pytorch_model.bin',map_location='cpu')\n",
    "\n",
    "# embeds = []\n",
    "# for k in md:\n",
    "#     if (k == 'bert.embeddings.word_embeddings.weight'):\n",
    "#         embeds = md[k]\n",
    "        \n",
    "X = np.array(embeds)\n",
    "\n",
    "words = []\n",
    "with open('../../models/NLI/vocab.txt', \"r\") as f:\n",
    "    words = f.readlines()\n",
    "    \n",
    "X_reduzido = X\n",
    "words_reduzido = words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização hierárquica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções pro plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_clustering(X_red, labels, title=None):\n",
    "    plt.rcParams['font.sans-serif'] = ['Source Han Sans TW', 'sans-serif']\n",
    "    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n",
    "    X_red = (X_red - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for i in range(X_red.shape[0]):\n",
    "        plt.text(X_red[i, 0], X_red[i, 1], str(words_reduzido[i]),\n",
    "                 color=plt.cm.nipy_spectral(labels[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title, size=17)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ward :\t190.63s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import manifold\n",
    "\n",
    "clustering = AgglomerativeClustering(linkage='ward', n_clusters=6000)\n",
    "t0 = time()\n",
    "model = clustering.fit(X_reduzido)\n",
    "print(\"%s :\\t%.2fs\" % ('ward', time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dict = {}\n",
    "# print(clusters_dict.items())\n",
    "for (w, x) in zip(words_reduzido, model.labels_):\n",
    "    if x in clusters_dict:\n",
    "        clusters_dict[x].append(w.replace('\\n', ''))\n",
    "    else: clusters_dict[x] = [w.replace('\\n', '')]\n",
    "\n",
    "# for i in clusters_dict:\n",
    "#     print(str(i) + \": \" + str(clusters_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.neighbors.nearest_centroid module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_reduzido' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-337a9bc3c27d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestCentroid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reduzido\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reduzido\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_reduzido' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X_reduzido, model.labels_)\n",
    "print(X_reduzido)\n",
    "\n",
    "clusters_vector = []\n",
    "\n",
    "\n",
    "for l in range(len(clf.centroids_)):\n",
    "    tsv_row = ''\n",
    "    tsv_row += str(clusters_dict[l]) + '\\t'\n",
    "\n",
    "    vector = clf.centroids_[l]\n",
    "    \n",
    "    for m in range(len(vector)):\n",
    "        tsv_row += str(vector[m].tolist()) + '\\t'\n",
    "\n",
    "    clusters_vector.append(tsv_row)\n",
    "\n",
    "with open('clustering-results/biobert/base-cased/6000clusters.tsv', \"w\") as f:\n",
    "    for e in clusters_vector:\n",
    "        print (e, file=f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotar dendograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "gstrf was called with invalid arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e67bfe9c6290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanifold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_red\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanifold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpectralEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reduzido\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \"\"\"\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    554\u001b[0m                                              \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                                              \u001b[0meigen_solver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigen_solver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                                              random_state=random_state)\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/manifold/_spectral_embedding.py\u001b[0m in \u001b[0;36mspectral_embedding\u001b[0;34m(adjacency, n_components, eigen_solver, random_state, eigen_tol, norm_laplacian, drop_first)\u001b[0m\n\u001b[1;32m    270\u001b[0m             _, diffusion_map = eigsh(\n\u001b[1;32m    271\u001b[0m                 \u001b[0mlaplacian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LM'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 tol=eigen_tol, v0=v0)\n\u001b[0m\u001b[1;32m    273\u001b[0m             \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffusion_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnorm_laplacian\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mOPinv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m                 Minv_matvec = get_OPinv_matvec(A, M, sigma,\n\u001b[0;32m-> 1642\u001b[0;31m                                                hermitian=True, tol=tol)\n\u001b[0m\u001b[1;32m   1643\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                 \u001b[0mOPinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_aslinearoperator_with_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOPinv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36mget_OPinv_matvec\u001b[0;34m(A, M, sigma, hermitian, tol)\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fast_spmatrix_to_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhermitian\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhermitian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mSpLuInv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m             return IterOpInv(_aslinearoperator_with_dtype(A),\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, M)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \"\"\"\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM_lu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/dsolve/linsolve.py\u001b[0m in \u001b[0;36msplu\u001b[0;34m(A, permc_spec, diag_pivot_thresh, relax, panel_size, options)\u001b[0m\n\u001b[1;32m    324\u001b[0m     return _superlu.gstrf(N, A.nnz, A.data, A.indices, A.indptr,\n\u001b[1;32m    325\u001b[0m                           \u001b[0mcsc_construct_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsc_construct_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                           ilu=False, options=_options)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: gstrf was called with invalid arguments"
     ]
    }
   ],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "X_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X_reduzido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustering(X_red, clustering.labels_, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(model.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
