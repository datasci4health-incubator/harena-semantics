{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means em BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# BERT_MODEL='../models/bert-base-cased'\n",
    "BERT_MODEL='../models/biobert'\n",
    "\n",
    "# BERT_MODEL='bert-base-cased'\n",
    "# BERT_MODEL='dmis-lab/biobert-base-cased-v1.1'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=False)\n",
    "model = BertModel.from_pretrained(BERT_MODEL, output_hidden_states = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model, tokenizer, text):\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    \n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        hidden_states = outputs[2]\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text):\n",
    "   \n",
    "    hidden_states = get_hidden_states(model, tokenizer, text)\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    \n",
    "    return token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"A 58-year-old African-American woman presents to the ER with episodic pressing/burning anterior chest pain that began two days earlier for the first time in her life. The pain started while she was walking, radiates to the back, and is accompanied by nausea, diaphoresis and mild dyspnea, but is not increased on inspiration. The latest episode of pain ended half an hour prior to her arrival. She is known to have hypertension and obesity. She denies smoking, diabetes, hypercholesterolemia, or a family history of heart disease. She currently takes no medications. Physical examination is normal. The EKG shows nonspecific changes.\"\n",
    "\n",
    "input_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "\n",
    "tokenized_input = tokenizer.tokenize(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_embeddings = []\n",
    "\n",
    "token_embeddings = get_embeddings(input_sentence)\n",
    "\n",
    "for token_emb in token_embeddings:\n",
    "    last_embedding_layer = token_emb[12]\n",
    "    last_embeddings.append(last_embedding_layer)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## salva os embeddings da sentença"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for l in range(len(last_embeddings)):\n",
    "    vector = last_embeddings[l]\n",
    "    tsv_row = ''\n",
    "    for m in range(len(vector)):\n",
    "        tsv_row += str(vector[m].tolist()) + '\\t'\n",
    "\n",
    "    vectors.append(tsv_row)\n",
    "\n",
    "with open('tsv_files/sentence_word_embeddings.tsv', \"w\") as f:\n",
    "    for e in vectors:\n",
    "        print (e, file=f)   \n",
    "        \n",
    "labels = []\n",
    "with open('tsv_files/sentence_labels.tsv', \"w\") as labels_file:\n",
    "    for label in tokenized_input:\n",
    "        print(label.replace(\"\\n\", \"\"), file=labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização com NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "\n",
    "NUM_CLUSTERS=10\n",
    "\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(last_embeddings, assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # words = list(model.wv.vocab)\n",
    "# i = 0\n",
    "# for word in tokenized_input:  \n",
    "#     print (word + \":\" + str(assigned_clusters[i]))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusterização com scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar de list  de tensores pra ndArray de n_input_tokens x 768\n",
    "import numpy as np\n",
    "\n",
    "aux_array = []\n",
    "for last_embedding in last_embeddings:\n",
    "    aux_array.append(last_embedding.numpy())\n",
    "X = np.array(aux_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id labels for input data\n",
      "[1 2 3 9 4 5 0 7 0 6 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\n",
      "-8.293188429571686e+16\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(X)\n",
    " \n",
    "cluster_labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    " \n",
    "print (\"Cluster id labels for input data\")\n",
    "print (cluster_labels)\n",
    "\n",
    "print (\"Score (Opposite of the value of X on the K-means objective which is Sum of distances of samples to their closest cluster center):\")\n",
    "print (kmeans.score(X))\n",
    " \n",
    "# silhouette_score = metrics.silhouette_score(X, labels, metric='euclidean')\n",
    " \n",
    "# print (\"Silhouette_score: \")\n",
    "# print (silhouette_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## salva os embeddings da sentença e dos clusters da sentença"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "labels = []\n",
    "\n",
    "i = 0\n",
    "for c in centroids:\n",
    "    tsv_row = ''\n",
    "    for cc in c:\n",
    "        tsv_row += str(cc.tolist()) + '\\t'\n",
    "    \n",
    "    vectors.append(tsv_row)\n",
    "    labels.append(\"Centroid cluster \" + str(i))\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "for l in range(len(last_embeddings)):\n",
    "    vector = last_embeddings[l]\n",
    "    tsv_row = ''\n",
    "    for m in range(len(vector)):\n",
    "        tsv_row += str(vector[m].tolist()) + '\\t'\n",
    "\n",
    "    vectors.append(tsv_row)\n",
    "\n",
    "for t_i in tokenized_input:\n",
    "    labels.append(t_i)\n",
    "    \n",
    "    \n",
    "with open('tsv_files/sentence_and_clusters_word_embeddings_aux.tsv', \"w\") as f:\n",
    "    for e in vectors:\n",
    "        print (e, file=f)   \n",
    "with open('tsv_files/sentence_and_clusters_labels_aux.tsv', \"w\") as labels_file:\n",
    "    for label in labels:\n",
    "        print(label.replace(\"\\n\", \"\"), file=labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
