{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l76VA9rNh9Qm",
    "outputId": "8ac2db91-f6f4-41f4-cc05-2e6495be5b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.8/site-packages (2.4.1)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.15.6)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.4.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.29.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (49.6.0.post20210108)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: keras in /opt/conda/lib/python3.8/site-packages (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.8/site-packages (from keras) (1.6.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.8/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.8/site-packages (from keras) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from seqeval) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.8/site-packages (from seqeval) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->seqeval) (2.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (1.8.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch) (1.19.5)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch) (3.7.4.3)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install tensorflow\n",
    "! pip install keras\n",
    "! pip install seqeval\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> cf8bd79e00e6469d726fc4893d491774a324d8a8
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMdjRIwMQL12",
    "outputId": "f89973f5-7149-466b-db7c-e1be09d0cc94"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XSfvE67jvdMY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "MODEL = 'bert-base-cased'\n",
    "# MODEL_PATH += MODEL\n",
    "\n",
    "# MODEL = ''\n",
    "OUTPUT_PATH = '../../models/word_embeddings/fine_tuned/NER/ACD_1epoch/jupyter_running/'\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "# OUTPUT_MODEL_PATH = OUTPUT_PATH + DATASET\n",
    "\n",
    "\n",
    "# DATASET = 'AnatEM-BC5CDR'\n",
    "DATASET = 'ACD'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjg9Q--zQImm"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVDKyBDH8lCj",
    "outputId": "e64a089a-bc6f-4aa5-8a30-7002e2aca2bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:  label\n",
      "B-Anatomy       6946\n",
      "B-Chemical      5203\n",
      "B-Disease       4182\n",
      "I-Anatomy       4891\n",
      "I-Chemical      1900\n",
      "I-Disease       2918\n",
      "O             245927\n",
      "dtype: int64\n",
      "\n",
      "Devel labels:  label\n",
      "B-Anatomy       2139\n",
      "B-Chemical      5347\n",
      "B-Disease       4246\n",
      "I-Anatomy       1416\n",
      "I-Chemical      1748\n",
      "I-Disease       2723\n",
      "O             158617\n",
      "dtype: int64\n",
      "\n",
      "Test labels:  label\n",
      "B-Anatomy       4616\n",
      "B-Chemical      5378\n",
      "B-Disease       4424\n",
      "I-Anatomy       3243\n",
      "I-Chemical      1628\n",
      "I-Disease       2737\n",
      "O             202684\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../../datasets/NER/'+ DATASET + '/train.tsv', delimiter='\\t', names=['word', 'label'], quoting=3, error_bad_lines=False)\n",
    "test = pd.read_csv('../../datasets/NER/'+ DATASET + '/test.tsv', delimiter='\\t', names=['word', 'label'], quoting=3, error_bad_lines=False)\n",
    "devel = pd.read_csv('../../datasets/NER/'+ DATASET + '/devel.tsv', delimiter='\\t', names=['word', 'label'], quoting=3, error_bad_lines=False)\n",
    "print('Train labels: ', train.groupby('label').size())\n",
    "print()\n",
    "print('Devel labels: ', devel.groupby('label').size())\n",
    "print()\n",
    "print('Test labels: ', test.groupby('label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ET9DHejiQImn",
    "outputId": "7c62ef39-9bd9-403f-95b7-644be6d11cd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word label\n",
      "0  Immunostaining     O\n",
      "1             and     O\n",
      "2        confocal     O\n",
      "3        analysis     O\n",
      "4             DNA     O\n",
      "5       labelling     O\n",
      "6             and     O\n",
      "7        staining     O\n",
      "8            with     O\n",
      "9               5     O\n"
     ]
    }
   ],
   "source": [
    "print(train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NG4IrGC87hGu"
   },
   "outputs": [],
   "source": [
    "# Separa o dataframe por PONTO-FINAL\n",
    "def separar_frases(dataframe):\n",
    "  sentences = []\n",
    "  labels = []\n",
    "\n",
    "  sentences_aux = []\n",
    "  labels_aux = []\n",
    "\n",
    "  inicio = True\n",
    "\n",
    "  for word, label in zip(dataframe.word.values, dataframe.label.values):\n",
    "    if inicio:\n",
    "        sentences_aux.append('[CLS]')\n",
    "        labels_aux.append('O')\n",
    "        inicio = False\n",
    "    sentences_aux.append(word)\n",
    "    labels_aux.append(label)\n",
    "\n",
    "    if (word == '.'):\n",
    "        sentences_aux.append('[SEP]')\n",
    "        labels_aux.append('O')\n",
    "        \n",
    "        sentences.append(sentences_aux)\n",
    "        labels.append(labels_aux)\n",
    "        \n",
    "        sentences_aux = []\n",
    "        labels_aux = []\n",
    "        inicio = True\n",
    "\n",
    "  return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LEgoZ4Xw93me"
   },
   "outputs": [],
   "source": [
    "train_sentences, train_labels = separar_frases(train)\n",
    "test_sentences, test_labels = separar_frases(test)\n",
    "devel_sentences, devel_labels = separar_frases(devel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3omHcaVkQImq"
   },
   "source": [
    "### Map tags to id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hhGbLgHTqJnD",
    "outputId": "4c872308-a93b-4d58-c474-e6317da1c778"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Anatomy', 'B-Chemical', 'B-Disease', 'I-Anatomy', 'I-Chemical', 'I-Disease', 'O', 'PAD']\n",
      "{'I-Chemical': 0, 'I-Anatomy': 1, 'B-Anatomy': 2, 'O': 3, 'B-Chemical': 4, 'B-Disease': 5, 'I-Disease': 6, 'PAD': 7}\n"
     ]
    }
   ],
   "source": [
    "tag_values = list(set(train[\"label\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "print(sorted(tag_values))\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTIaNOu2QIms"
   },
   "source": [
    "### Prepare sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "I6gyI1yHwIAf",
    "outputId": "465898f8-3704-4d5d-8cea-167cd75b6907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "print('Dispositivo:', device)\n",
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eOW0Yo31QImu"
   },
   "outputs": [],
   "source": [
    "# Configura o tamanho máximo da sentenca e tamanho do batch de processamento\n",
    "MAX_LEN = 75\n",
    "# bs = 32\n",
    "bs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib7W9dHGQImv"
   },
   "source": [
    "## Tokenizar a entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DsJ8sdY-o_-K"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "#         print('sentence ', word)\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "#         print(word)\n",
    "#         break\n",
    "        tokenized_word = tokenizer.tokenize(str(word))\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        if label.startswith(\"B\"):\n",
    "            labels.extend([label])\n",
    "            new_label = \"I-\" + label[2:]\n",
    "\n",
    "            labels.extend([new_label] * (n_subwords-1))\n",
    "        else:\n",
    "            labels.extend([label] * n_subwords)\n",
    "\n",
    "\n",
    "    return tokenized_sentence, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWHKU_MVpMtP",
    "outputId": "b05a8e5f-28bc-45fd-cc61-6afdbcc5238d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['[CLS]', 'I', '##mm', '##uno', '##sta', '##ining', 'and', 'con', '##fo', '##cal', 'analysis', 'DNA', 'label', '##ling', 'and', 'stain', '##ing', 'with', '5', '-', 'br', '##omo', '-', '2', \"'\", '-', 'de', '##ox', '##yu', '##rid', '##ine', '(', 'B', '##rd', '##U', 'label', '##ling', 'and', 'detection', 'kit', 'I', ';', 'Bo', '##eh', '##ring', '##er', 'Mann', '##heim', ',', 'Germany', ')', 'was', 'performed', 'according', 'to', 'the', 'manufacturer', \"'\", 's', 'instructions', '.', '[SEP]'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL, do_lower_case=False)\n",
    "\n",
    "train_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train_sentences, train_labels)\n",
    "]\n",
    "test_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(test_sentences, test_labels)\n",
    "]\n",
    "devel_tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(devel_sentences, devel_labels)\n",
    "]\n",
    "\n",
    "print(train_tokenized_texts_and_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "W6lU6JA-QImw"
   },
   "outputs": [],
   "source": [
    "#Antigo\n",
    "# train_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_tokenized_texts],\n",
    "#                           maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "#                           truncating=\"post\", padding=\"post\")\n",
    "# test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts],\n",
    "#                           maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "#                           truncating=\"post\", padding=\"post\")\n",
    "# devel_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in devel_tokenized_texts],\n",
    "#                           maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "#                           truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2J-nQ7ZCpRIz",
    "outputId": "25bf98e8-fca8-46e9-97ba-386231d1ebca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,   146,  6262, 26761,  8419, 16534,  1105, 14255, 14467,\n",
       "        7867,  3622,  5394,  3107,  1979,  1105, 24754,  1158,  1114,\n",
       "         126,   118,  9304, 18445,   118,   123,   112,   118,  1260,\n",
       "       10649,  9379, 10132,  2042,   113,   139,  2956,  2591,  3107,\n",
       "        1979,  1105, 11432, 11622,   146,   132,  9326, 10486,  3384,\n",
       "        1200, 10852,  6797,   117,  1860,   114,  1108,  1982,  2452,\n",
       "        1106,  1103,  7400,   112,   188,  7953,   119,   102,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refatorado\n",
    "train_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(token[0]) for token in train_tokenized_texts_and_labels],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(token[0]) for token in test_tokenized_texts_and_labels],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "devel_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(token[0]) for token in devel_tokenized_texts_and_labels],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "train_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uHVyNqbxQImx"
   },
   "outputs": [],
   "source": [
    "## presentação com PADids da tag das tags em tamanho de 75 (5 é PAD, 0 É O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMrD5IDoQImx",
    "outputId": "c3012298-452f-43b5-aa05-d709350d7b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-Chemical': 1, 'I-Anatomy': 2, 'B-Disease': 3, 'B-Anatomy': 4, 'I-Chemical': 5, 'I-Disease': 6, 'PAD': 7}\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7]\n"
     ]
    }
   ],
   "source": [
    "train_tags = pad_sequences([[tag2idx.get(l) for l in lab[1]] for lab in train_tokenized_texts_and_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "test_tags = pad_sequences([[tag2idx.get(l) for l in lab[1]] for lab in test_tokenized_texts_and_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "devel_tags = pad_sequences([[tag2idx.get(l) for l in lab[1]] for lab in devel_tokenized_texts_and_labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "print(tag2idx)\n",
    "print(train_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luFIMmvbQImy"
   },
   "source": [
    "> train_tag[i] é a fita de tags normalizada (i.e., adicionando PAD tokens) para um tamanho = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBeGAhn7pVDY",
    "outputId": "bd26cce1-e2b9-463b-84f0-fb406e96d156"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-06910a082d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevel_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevel_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_attention_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-06910a082d23>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevel_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevel_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_attention_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-06910a082d23>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevel_attention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevel_input_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_attention_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_attention_masks = [[float(i != 0.0) for i in ii] for ii in train_input_ids]\n",
    "test_attention_masks = [[float(i != 0.0) for i in ii] for ii in test_input_ids]\n",
    "devel_attention_masks = [[float(i != 0.0) for i in ii] for ii in devel_input_ids]\n",
    "print(train_attention_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEDCteMHQImz"
   },
   "source": [
    "> train_attention_masks[i]  é o vetor que diz ao BERT onde prestar atençao "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CARIrVeCpW-J",
    "outputId": "2d93fb5a-40e1-4d78-b845-9514ecb0399e"
   },
   "outputs": [],
   "source": [
    "# tr_inputs, val_inputs, tr_tags, val_tags, tr_masks, val_masks = shuffle(train_input_ids, test_input_ids, train_tags, test_tags, train_attention_masks, test_attention_masks, random_state=2020)\n",
    "# devel_inputs, devel_tags, devel_masks = shuffle(devel_input_ids, devel_tags, devel_attention_masks, random_state=2020)\n",
    "# # pŕint(tr_inputs[0])\n",
    "print(train_input_ids[0])\n",
    "print(train_tags[0])\n",
    "tr_inputs, tr_tags, tr_masks = shuffle(train_input_ids, train_tags, train_attention_masks, random_state=2020)\n",
    "val_inputs, val_tags, val_masks = shuffle(test_input_ids, test_tags, test_attention_masks, random_state=2020)\n",
    "devel_inputs, devel_tags, devel_masks = shuffle(devel_input_ids, devel_tags, devel_attention_masks, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXPueD-QDqvb"
   },
   "outputs": [],
   "source": [
    "# converter para ternsores\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "devel_inputs = torch.tensor(devel_inputs)\n",
    "devel_tags = torch.tensor(devel_tags)\n",
    "devel_masks = torch.tensor(devel_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHnjuT6HDtLU"
   },
   "outputs": [],
   "source": [
    "# construir os DataLoader\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n",
    "\n",
    "devel_data = TensorDataset(devel_inputs, devel_masks, devel_tags)\n",
    "devel_sampler = SequentialSampler(devel_data)\n",
    "devel_dataloader = DataLoader(devel_data, sampler=devel_sampler, batch_size=bs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "bL9VsQfTDxUz",
    "outputId": "eef1c25f-0a6e-4701-c5f3-37770ae006f8"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "transformers.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLpfqVwjDzSt",
    "outputId": "5fecfeab-05da-4f61-ba6e-cd249652f3d7"
   },
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\n",
    "    MODEL,\n",
    "    num_labels=len(tag2idx),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qevkb5MiD_ig",
    "outputId": "d5234aaa-5304-4565-daff-692d305d4231"
   },
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ki12BV1OEKvM"
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqKCYAhhEMtK"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 1\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qz5pu2hDETOz"
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKfqRhSnEUyi",
    "outputId": "9869d42a-36cd-4189-b7c7-caaaab5ca582"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm, trange\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "validation_accuracy, validation_f1 = [], []\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "#    1 batch = 32 sentenças\n",

    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print()\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "#     A cada 32 sentenças\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(predictions, true_labelsbels)\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score([pred_tags], [valid_tags])))\n",
    "    print()\n",
    "    validation_accuracy.append(accuracy_score(pred_tags, valid_tags))\n",
    "    validation_f1.append(f1_score([pred_tags], [valid_tags]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "xbd3Og2JEXrO",
    "outputId": "8a814cf7-f177-4fbd-97ae-f3874dc47ae4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Learning curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('learning-curves')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "nN1RZIJkPlGB",
    "outputId": "cd896a4a-9344-4287-9125-668d2e303dde"
   },
   "outputs": [],
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(validation_accuracy, 'b-o', label=\"Validation accuracy\")\n",
    "plt.plot(validation_f1, 'r-o', label=\"Validation f1\")\n",
    "# Label the plot.\n",
    "plt.title(\"Score curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfLf44maNREm",
    "outputId": "ee0b8472-a5f9-44ad-a0fc-6e0900bb5f86"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# Reset the validation loss for this epoch.\n",
    "devel_loss, devel_accuracy = 0, 0\n",
    "nb_devel_steps, nb_devel_examples = 0, 0\n",
    "predictions , true_labels = [], []\n",
    "for batch in devel_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients,\n",
    "    # saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have not provided labels.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "    # Move logits and labels to CPU\n",
    "    logits = outputs[1].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Calculate the accuracy for this batch of test sentences.\n",
    "    devel_loss += outputs[0].mean().item()\n",
    "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "    true_labels.extend(label_ids)\n",
    "\n",
    "devel_loss = devel_loss / len(devel_dataloader)\n",
    "print(\"Devel loss: {}\".format(devel_loss))\n",
    "pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                              for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                              for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "print(\"Devel Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "print(\"Devel F1-Score: {}\".format(f1_score([pred_tags], [valid_tags])))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leFxHY5uEZwM",
    "outputId": "3efbeec0-0d68-4364-b3bc-702ae56d7ce6"
   },
   "outputs": [],
   "source": [
    "# test_sentence = \"This expression of NT-3 in supporting cells in embryos and neonates may even preserve in Brn3c null mutants the numerous spiral sensory neurons in the apex of 8-day old animals.Hypertensive pneumothorax is more common in tall and thin young adults (primary pneumothorax) or in patients with chronic pulmonary diseases or chest trauma (secondary pneumothorax).\"\n",
    "# test_sentence = 'The cardiac and pulmonary auscultation are normal; chest pain does not worse with palpation of the thorax; there is no jugular stasis nor lower limb edema.'\n",
    "\n",
    "# test_sentence = 'The pain is continuous and is located just in the middle of my chest, worsening when I breathe and when I lay down on my bed, I suffer from arterial hypertension and smoke 20 cigarettes every day.'\n",
    "# test_sentence = 'Sepsis is the second most common cause of death in non-coronary intensive care units and the tenth leading cause of death overall in high-income countries. During the past two decades, the incidence of sepsis has increased annually by 9% to reach 240 per 100 000 population in the USA by 2000.'\n",
    "# test_sentence = 'The true incidence of sepsis in any given country is unknown. The reported incidence is dependent on the specific definition used, the infecting organism, the reporting mechanism (such as the use of the International Classification of Diseases-9 coding systems) and the requirement for either organ support or intensive care. These factors result in marked differences between estimates and discrete geographical locations. Most data describing the incidence of sepsis are from high-income countries, where 2.8 million deaths per year are attributable to sepsis. In 2001, Angus and colleagues reported in, the USA the, that incidence of severe sepsis was more than 750 000 cases per annum (300 cases per 100 000 population), equivalent to 2.26 cases per 100 hospital discharges. In the UK, the reported prevalence of sepsis in ICU-derived cohorts is 27% of all ICU admissions, whereas the prevalence is 12% in the USA. 14 This difference could partly be explained by the sub stantially greater numbers of ICU beds available in the USA than in the UK, and thus the differing triage patterns and admission criteria. It is also possible that, in institutions where clinical staff are trained in sepsis recognition, the previous use of the less-specific SIRS criteria could have led to an over-reporting of sepsis cases. Overall, however, there is probably a substantial under-reporting of the incidence of sepsis and with an ageing population, the incidence will continue to increase. This pattern might be further accentuated by campaigns to increase the awareness of and screening for the condition. Except for maternal and neonatal sepsis, the condition is usually considerably under-reported in the global burden of disease statistics. The true scale of the problem is probably much higher than what has been reported. Data suggest that sepsis contributes to between a third and a half of all in-hospital deaths in the USA. Although these data represent the incidence of sepsis in high-resource countries, most deaths due to sepsis happen in low-resource countries, where the exact incidence of sepsis is difficult to accurately estimate. The available literature suggests that an estimated 90% of worldwide deaths from chest infections occur in low-resource settings; about 70% of the 9 million deaths due to chest infections in neonates and infants are associated with sepsis, and most cases occur in Asia and Africa.'\n",
    "\n",
    "# test_sentence = \" Her eye is green. A 58-year-old African-American woman presents to the ER with episodic pressing/burning anterior chest pain that began two days earlier for the first time in her life. The pain started while she was walking, radiates to the eyes, and is accompanied by nausea, diaphoresis and mild dyspnea, but is not increased on inspiration. The latest episode of pain ended half an hour prior to her arrival. She is known to have hypertension and obesity. She denies smoking, diabetes, hypercholesterolemia, or a family history of heart disease. She currently takes no medications. Physical examination is normal. The EKG shows nonspecific changes. Alprazolan was administred.\"\n",
    "# test_sentence = 'My eye is blue. I feel nausea. '\n",
    "test_sentence = 'Doctor, I am feeling chest pain since yesterday. The pain is continuous and is located just in the middle of my chest, worsening when I breathe and when I lay down on my bed. I suffer from arterial hypertension and smoke 20 cigarettes every day. My father had a “heart attack” at my age and I am very worried about it.'\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "# print(output)\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "# join bpe split tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    print(token, label_idx)\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(tag_values[label_idx])\n",
    "        new_tokens.append(token)\n",
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t\\t{}\".format(label, token))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_muK6HHQQIm6",
    "outputId": "bd6f95b7-d561-4dad-b0f1-69b77c90d481"
   },
   "outputs": [],
   "source": [
    "label_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5URn2cTQIm6"
   },
   "outputs": [],
   "source": [
    "# # label_list = [\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"[CLS]\", \"[SEP]\"]\n",
    "\n",
    "# import json, os\n",
    "\n",
    "# # Save a trained model and the associated configuration\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "# model_to_save.save_pretrained('model')\n",
    "# tokenizer.save_pretrained('model')\n",
    "# label_map = {i : label for i, label in enumerate(tag_values,1)}        \n",
    "# model_config = {\"bert_model\":\"bert-base-cased\",\"do_lower\":False,\"max_seq_length\":128,\"num_labels\":len(tag_values)+1,\"label_map\":label_map}\n",
    "# json.dump(model_config,open(os.path.join(\"model\",\"model_config.json\"),\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBplGsRIdAAu",
    "outputId": "ef96699c-79ef-499a-d62c-90e2084fc870"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "tokenizer.save_pretrained(OUTPUT_MODEL_PATH)\n",
    "\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MO0JnqL_QIm6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "biobert_ner.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
