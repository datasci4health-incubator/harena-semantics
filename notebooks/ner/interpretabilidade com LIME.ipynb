{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cross-timing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eli5\n",
      "  Downloading eli5-0.11.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 922 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from eli5) (1.6.1)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: attrs>16.0.0 in /opt/conda/lib/python3.8/site-packages (from eli5) (20.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/lib/python3.8/site-packages (from eli5) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from eli5) (1.19.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from eli5) (2.11.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from eli5) (1.15.0)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.16-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.20->eli5) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.8/site-packages (from jinja2->eli5) (1.1.1)\n",
      "Installing collected packages: tabulate, graphviz, eli5\n",
      "Successfully installed eli5-0.11.0 graphviz-0.16 tabulate-0.8.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "formal-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def get_labels(path: str) -> List[str]:\n",
    "    with open(path, \"r\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "        labels = [i if i != 'O' else 'O' for i in labels]\n",
    "    if \"O\" not in labels:\n",
    "        labels = [\"O\"] + labels\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "national-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer)\n",
    "\n",
    "\n",
    "# MODEL_NAME_OR_PATH =  'dmis-lab/biobert-base-cased-v1.1'\n",
    "MODEL_NAME_OR_PATH =  '../../models/word_embeddings/fine_tuned/NER/ACD_15epochs'\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH,\n",
    "    do_lower_case=False\n",
    ")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "labels = [value for k, value in config.id2label.items()]\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "democratic-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\t\tO\n",
      "In\t\tO\n",
      "two\t\tO\n",
      "of\t\tO\n",
      "the\t\tO\n",
      "three\t\tO\n",
      "deaths\t\tB-Disease\n",
      "probably\t\tO\n",
      "associated\t\tO\n",
      "with\t\tO\n",
      "ketoconazole\t\tB-Chemical\n",
      "treatment\t\tO\n",
      "the\t\tO\n",
      "drug\t\tO\n",
      "had\t\tO\n",
      "been\t\tO\n",
      "continued\t\tO\n",
      "after\t\tO\n",
      "the\t\tO\n",
      "onset\t\tO\n",
      "of\t\tO\n",
      "jaundice\t\tB-Disease\n",
      "and\t\tO\n",
      "other\t\tO\n",
      "symptoms\t\tO\n",
      "of\t\tO\n",
      "hepatitis\t\tB-Disease\n",
      ".\t\tO\n",
      "[SEP]\t\tO\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "test_sentence = 'In two of the three deaths probably associated with ketoconazole treatment the drug had been continued after the onset of jaundice and other symptoms of hepatitis.'\n",
    "\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "input_ids = torch.tensor([tokenized_sentence])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(labels[label_idx])\n",
    "        new_tokens.append(token)\n",
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t\\t{}\".format(token, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "artistic-green",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In two of the three deaths probably associated with ketoconazole treatment the drug had been continued after the onset of jaundice and other symptoms of hepatitis.\n",
      "('In two of the UNK deaths probably associated with ketoconazole treatment the drug had been UNK after UNK onset UNK jaundice UNK other UNK of hepatitis.', 'In two UNK UNK three deaths UNK associated UNK ketoconazole treatment UNK UNK UNK been UNK UNK UNK onset of UNK and other symptoms of hepatitis.', 'In UNK of the UNK deaths UNK UNK with UNK treatment the drug UNK UNK UNK UNK the UNK of UNK UNK other UNK UNK hepatitis.', 'In two of the three deaths probably associated with ketoconazole UNK the drug had been continued after the onset of jaundice and other symptoms of hepatitis.')\n"
     ]
    }
   ],
   "source": [
    "from eli5.lime import TextExplainer\n",
    "from eli5.lime.samplers import MaskingTextSampler\n",
    "\n",
    "sampler = MaskingTextSampler(\n",
    "    replacement=\"UNK\",\n",
    "    max_replace=0.7,\n",
    "    token_pattern=None,\n",
    "    bow=False\n",
    ")\n",
    "\n",
    "samples, similarity = sampler.sample_near(test_sentence, n_samples=4)\n",
    "print(test_sentence)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eight-bottle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aqui\n",
      "<class 'transformers.modeling_outputs.TokenClassifierOutput'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-24c5c6d89518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mpredict_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mpredict_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m te = TextExplainer(\n",
      "\u001b[0;32m<ipython-input-127-24c5c6d89518>\u001b[0m in \u001b[0;36mget_predict_function\u001b[0;34m(self, word_index)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p_array '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0man_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mexplainer_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNERExplainerGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-24c5c6d89518>\u001b[0m in \u001b[0;36mpredict_func\u001b[0;34m(input_ids)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0man_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p_array '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0man_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "class NERExplainerGenerator(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "       \n",
    "        \n",
    "#     def _preprocess(self, texts):\n",
    "#         X = [[self.word2idx.get(w, self.word2idx[\"UNK\"]) for w in t.split()]\n",
    "#              for t in texts]\n",
    "#         X = pad_sequences(maxlen=self.max_len, sequences=X,\n",
    "#                           padding=\"post\", value=self.word2idx[\"PAD\"])\n",
    "#         return X\n",
    "    \n",
    "    def get_predict_function(self, word_index):\n",
    "        def predict_func(input_ids):\n",
    "#             X = self._preprocess(texts)\n",
    "            print('aqui')\n",
    "#             print(input_ids[0][:])\n",
    "            p = self.model(input_ids)\n",
    "            p_array = p['logits'][0][word_index]\n",
    "            print(type(p))\n",
    "            print(type(p['logits']))\n",
    "\n",
    "            an_array = p_array.eval(session=tf.compat.v1.Session())\n",
    "            print('p_array ', an_array)\n",
    "            return p['logits'][0][word_index]\n",
    "        return predict_func(input_ids)\n",
    "\n",
    "explainer_generator = NERExplainerGenerator(model)\n",
    "word_index = 4\n",
    "\n",
    "predict_func = explainer_generator.get_predict_function(word_index=word_index)\n",
    "predict_func\n",
    "te = TextExplainer(\n",
    "    sampler=sampler,\n",
    "    position_dependent=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "te.fit(test_sentence, predict_func)\n",
    "\n",
    "# te.explain_prediction(\n",
    "#     target_names=list(explainer_generator.idx2tag.values()),\n",
    "#     top_targets=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-philip",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
