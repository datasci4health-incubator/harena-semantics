{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compact-tribe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (4.8.2)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (1.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers) (2021.7.6)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers) (4.61.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (1.9.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch) (3.10.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torchvision) (1.21.0)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision) (8.3.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "silver-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = 'fine_tuned/NER/ACD_1epoch'\n",
    "# MODEL = 'fine_tuned/NER/ACD_5epochs'\n",
    "# MODEL = 'fine_tuned/NER/ACD_10epochs'\n",
    "\n",
    "# MODEL = 'fine_tuned/NER/ACD'\n",
    "# MODEL = 'ACD(overfited)'\n",
    "\n",
    "# MODEL_PATH = '../../models/' + MODEL\n",
    "\n",
    "MODEL_PATH = 'fagner/envoy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "young-trustee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-Anatomy', 1: 'B-Chemical', 2: 'B-Disease', 3: 'I-Anatomy', 4: 'I-Chemical', 5: 'I-Disease', 6: 'O'}\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "\n",
    "labels = []\n",
    "labels = [value for k, value in config.id2label.items()]\n",
    "\n",
    "print(config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "attractive-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForTokenClassification, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH, do_lower_case=False)\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_labels=len(labels),\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "tamil-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa o dataframe por PONTO-FINAL\n",
    "def separar_frases(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "instructional-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    print(sentence)\n",
    "    tokenized_sentence = []\n",
    "    for word in sentence:\n",
    "#         print('sentence ', word)\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "#         print(word)\n",
    "#         break\n",
    "        tokenized_word = tokenizer.tokenize(str(word))\n",
    "        print(tokenized_word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        \n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "#         # Add the same label to the new list of labels `n_subwords` times\n",
    "#         if label.startswith(\"B\"):\n",
    "#             labels.extend([label])\n",
    "#             new_label = \"I-\" + label[2:]\n",
    "\n",
    "#             labels.extend([new_label] * (n_subwords-1))\n",
    "#         else:\n",
    "#             labels.extend([label] * n_subwords)\n",
    "\n",
    "\n",
    "    return tokenized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "objective-methodology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 0 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 0 6 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 0 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 0 5 6 6 6 6 6 6 6 6 6\n",
      " 0 3 6 6 6 6 0 3 3 6 6 6 6 0 3 2 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6]\n",
      "aqui\n",
      " @ nurse Doctor , we have a man ( 51 years old ) who entered the emergency department reporting {chest}(Anatomy) {pain}(Disease) . His vital signs are ABP : 144x92mmHG ; HR : 78bpm ; RR : 21rpm ; Temp : 37oC ; O2Sat : 98 % . @ patient Doctor , I am feeling {chest}(Anatomy) {pain}(Disease) since yesterday . The pain is continuous and is located just in the middle of my {chest}(Anatomy) , worsening when I breathe and when I lay down on my bed . I suffer from {arterial}(Anatomy) {hypertension}(Disease) and smoke 20 cigarettes every day . My father had a “ {heart}(Anatomy) attack ” at my age and I am very worried about it . I use sertralina . PHYSICAL EXAMINATION The cardiac and pulmonary auscultation are normal ; {chest}(Anatomy) {pain}(Disease) does not worse with palpation of the {thorax}(Anatomy) ; there is no {jugular}(Anatomy) stasis nor {lower limb}(Anatomy) {edema}(Disease) . @ system What do you want to do ? Generate hypothesis More information Call the supervisor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "test_sentence = \"\"\" @nurse  Doctor, we have a man (51 years old) who entered the emergency department reporting chest pain. His vital signs are ABP: 144x92mmHG; HR: 78bpm; RR: 21rpm; Temp: 37oC; O2Sat: 98%.\n",
    "\n",
    "@patient  Doctor, I am feeling chest pain since yesterday. The pain is continuous and is located just in the middle of my chest, worsening when I breathe and when I lay down on my bed. I suffer from arterial hypertension and smoke 20 cigarettes every day. My father had a “heart attack” at my age and I am very worried about it.\n",
    "I use sertralina.\n",
    "PHYSICAL EXAMINATION\n",
    "\n",
    "The cardiac and pulmonary auscultation are normal; chest pain does not worse with palpation of the thorax; there is no jugular stasis nor lower limb edema.\n",
    "\n",
    "@system  What do you want to do?\n",
    "\n",
    "    Generate hypothesis\n",
    "    More information\n",
    "    Call the supervisor \"\"\"\n",
    "\n",
    "\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "\n",
    "# input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "input_ids = torch.tensor([tokenized_sentence])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "versum = \"\"\n",
    "\n",
    "print(label_indices[0])\n",
    "# print(len(tokens))\n",
    "\n",
    "# for token, label_idx in zip(tokens, label_indices[0]):\n",
    "entitying = False\n",
    "change_of_label = False\n",
    "current_label = \"\"\n",
    "for idx, (token, label_idx) in enumerate(zip(tokens, label_indices[0])):\n",
    "#     print(token)    \n",
    "#     print(label_idx)\n",
    "#     print(labels[label_idx])\n",
    "\n",
    "#     print()\n",
    "    if (token == \"[CLS]\" or token == \"[SEP]\"):\n",
    "        continue\n",
    "        \n",
    "    if token.startswith(\"##\"):\n",
    "        sub_token = True\n",
    "        versum = versum + token[2:]\n",
    "#         new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        if (labels[label_idx].startswith(\"B\")):\n",
    "            if entitying:\n",
    "                versum += '}(' + entity +')'\n",
    "                \n",
    "            entitying = True\n",
    "            versum = versum + ' {' + token\n",
    "#             print(versum)\n",
    "            entity = labels[label_idx][2:]\n",
    "            \n",
    "        \n",
    "        if labels[label_idx].startswith(\"O\") :\n",
    "            if entitying:\n",
    "#                 print(entity)\n",
    "                versum = versum + '}(' + entity +') ' + token\n",
    "            else:\n",
    "                versum = versum + ' ' + token\n",
    "            entitying = False\n",
    "\n",
    "                \n",
    "        if labels[label_idx].startswith(\"I\") :\n",
    "            if previous_label[2:] != labels[label_idx][2:]:\n",
    "                versum += '}(' + entity +') {'\n",
    "                entity = labels[label_idx][2:]\n",
    "                change_of_label = True\n",
    "#                 entitying = False\n",
    "            else: change_of_label = False\n",
    "                \n",
    "#             print(labels[label_idx][2:])\n",
    "#             if entitying:\n",
    "            if change_of_label:\n",
    "                versum += token\n",
    "                entitying = True\n",
    "            else:\n",
    "    #       print(entity)\n",
    "    #                 versum = versum + '}(' + entity +')'\n",
    "                print('aqui')\n",
    "                entitying = True\n",
    "                versum += ' ' + token\n",
    "\n",
    "#             else:\n",
    "#                 versum = versum + ' ' + token\n",
    "    previous_label = labels[label_idx]\n",
    "        \n",
    "#             new_labels.append(labels[label_idx])\n",
    "#             new_tokens.append(token)\n",
    "#     print(versum)\n",
    "\n",
    "    \n",
    "print(versum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a59b91b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\t\tO\n",
      "@\t\tO\n",
      "nurse\t\tO\n",
      "Doctor\t\tO\n",
      ",\t\tO\n",
      "we\t\tO\n",
      "have\t\tO\n",
      "a\t\tO\n",
      "man\t\tO\n",
      "(\t\tO\n",
      "51\t\tO\n",
      "years\t\tO\n",
      "old\t\tO\n",
      ")\t\tO\n",
      "who\t\tO\n",
      "entered\t\tO\n",
      "the\t\tO\n",
      "emergency\t\tO\n",
      "department\t\tO\n",
      "reporting\t\tO\n",
      "chest\t\tB-Anatomy\n",
      "pain\t\tI-Disease\n",
      ".\t\tO\n",
      "His\t\tO\n",
      "vital\t\tO\n",
      "signs\t\tO\n",
      "are\t\tO\n",
      "ABP\t\tO\n",
      ":\t\tO\n",
      "144x92mmHG\t\tO\n",
      ";\t\tO\n",
      "HR\t\tO\n",
      ":\t\tO\n",
      "78bpm\t\tO\n",
      ";\t\tO\n",
      "RR\t\tO\n",
      ":\t\tO\n",
      "21rpm\t\tO\n",
      ";\t\tO\n",
      "Temp\t\tO\n",
      ":\t\tO\n",
      "37oC\t\tO\n",
      ";\t\tO\n",
      "O2Sat\t\tO\n",
      ":\t\tO\n",
      "98\t\tO\n",
      "%\t\tO\n",
      ".\t\tO\n",
      "@\t\tO\n",
      "patient\t\tO\n",
      "Doctor\t\tO\n",
      ",\t\tO\n",
      "I\t\tO\n",
      "am\t\tO\n",
      "feeling\t\tO\n",
      "chest\t\tB-Anatomy\n",
      "pain\t\tI-Disease\n",
      "since\t\tO\n",
      "yesterday\t\tO\n",
      ".\t\tO\n",
      "The\t\tO\n",
      "pain\t\tO\n",
      "is\t\tO\n",
      "continuous\t\tO\n",
      "and\t\tO\n",
      "is\t\tO\n",
      "located\t\tO\n",
      "just\t\tO\n",
      "in\t\tO\n",
      "the\t\tO\n",
      "middle\t\tO\n",
      "of\t\tO\n",
      "my\t\tO\n",
      "chest\t\tB-Anatomy\n",
      ",\t\tO\n",
      "worsening\t\tO\n",
      "when\t\tO\n",
      "I\t\tO\n",
      "breathe\t\tO\n",
      "and\t\tO\n",
      "when\t\tO\n",
      "I\t\tO\n",
      "lay\t\tO\n",
      "down\t\tO\n",
      "on\t\tO\n",
      "my\t\tO\n",
      "bed\t\tO\n",
      ".\t\tO\n",
      "I\t\tO\n",
      "suffer\t\tO\n",
      "from\t\tO\n",
      "arterial\t\tB-Anatomy\n",
      "hypertension\t\tI-Disease\n",
      "and\t\tO\n",
      "smoke\t\tO\n",
      "20\t\tO\n",
      "cigarettes\t\tO\n",
      "every\t\tO\n",
      "day\t\tO\n",
      ".\t\tO\n",
      "My\t\tO\n",
      "father\t\tO\n",
      "had\t\tO\n",
      "a\t\tO\n",
      "“\t\tO\n",
      "heart\t\tB-Anatomy\n",
      "attack\t\tO\n",
      "”\t\tO\n",
      "at\t\tO\n",
      "my\t\tO\n",
      "age\t\tO\n",
      "and\t\tO\n",
      "I\t\tO\n",
      "am\t\tO\n",
      "very\t\tO\n",
      "worried\t\tO\n",
      "about\t\tO\n",
      "it\t\tO\n",
      ".\t\tO\n",
      "I\t\tO\n",
      "use\t\tO\n",
      "sertralina\t\tO\n",
      ".\t\tO\n",
      "PHYSICAL\t\tO\n",
      "EXAMINATION\t\tO\n",
      "The\t\tO\n",
      "cardiac\t\tO\n",
      "and\t\tO\n",
      "pulmonary\t\tO\n",
      "auscultation\t\tO\n",
      "are\t\tO\n",
      "normal\t\tO\n",
      ";\t\tO\n",
      "chest\t\tB-Anatomy\n",
      "pain\t\tI-Disease\n",
      "does\t\tO\n",
      "not\t\tO\n",
      "worse\t\tO\n",
      "with\t\tO\n",
      "palpation\t\tO\n",
      "of\t\tO\n",
      "the\t\tO\n",
      "thorax\t\tB-Anatomy\n",
      ";\t\tO\n",
      "there\t\tO\n",
      "is\t\tO\n",
      "no\t\tO\n",
      "jugular\t\tB-Anatomy\n",
      "stasis\t\tO\n",
      "nor\t\tO\n",
      "lower\t\tB-Anatomy\n",
      "limb\t\tI-Anatomy\n",
      "edema\t\tB-Disease\n",
      ".\t\tO\n",
      "@\t\tO\n",
      "system\t\tO\n",
      "What\t\tO\n",
      "do\t\tO\n",
      "you\t\tO\n",
      "want\t\tO\n",
      "to\t\tO\n",
      "do\t\tO\n",
      "?\t\tO\n",
      "Generate\t\tO\n",
      "hypothesis\t\tO\n",
      "More\t\tO\n",
      "information\t\tO\n",
      "Call\t\tO\n",
      "the\t\tO\n",
      "supervisor\t\tO\n",
      "[SEP]\t\tO\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# test_sentence = 'Doctor, I am feeling chest pain since yesterday. The pain is continuous and is located just in the middle of my chest, worsening when I breathe and when I lay down on my bed. I suffer from arterial hypertension and smoke 20 cigarettes every day. My father had a “heart attack” at my age and I am very worried about it.'\n",
    "# test_sentence = \" Her eye is green. A 58-year-old African-American woman presents to the ER with episodic pressing/burning anterior chest pain that began two days earlier for the first time in her life. The pain started while she was walking, radiates to the eyes, and is accompanied by nausea, diaphoresis and mild dyspnea, but is not increased on inspiration. The latest episode of pain ended half an hour prior to her arrival. She is known to have hypertension and obesity. She denies smoking, diabetes, hypercholesterolemia, or a family history of heart disease. She currently takes no medications. Physical examination is normal. The EKG shows nonspecific changes. Alprazolan was administred.\"\n",
    "# test_sentence = 'Recently, we found that therapy with selegiline and L-dopa was associated with selective systolic orthostatic hypotension which was abolished by withdrawal of selegiline. '\n",
    "# test_sentence = 'The cells were examined in a Zeiss LSM 510 laser scanning microscope equipped with a Plan - Apochromate 63x/1.4 oil immersion objective.'\n",
    "# test_sentence = 'Torsade de pointes ventricular tachycardia during  low dose intermittent dobutamine treatment in a patient with dilated cardiomyopathy and congestive heart failure.'\n",
    "# test_sentence = 'A DNA molecule'\n",
    "# test_sentence = '(a) Schematic drawing of the magnetic tweezers.'\n",
    "# test_sentence = 'Doctor, I am feeling a very strong chest pain in the middle of my chest. I have arterial hypertension but I don’t take my medication every day.'\n",
    "# test_sentence = 'Doctor, I am feeling a very strong chest pain in the middle of my chest. It has started 2 hours ago, but its intensity isn’t decreasing. It started suddenly and I am also feeling pain in my back and my neck. The pain is continuous, and it did not worse when I am breathing. I have arterial hypertension but I don’t take my medication every day, I am sorry! I don’t smoke, and I don’t know If I have other diseases. My father has also arterial hypertension.'\n",
    "# test_sentence = 'Doctor, I am feeling a very strong chest pain in the middle of my chest. It started suddenly and I am also feeling pain in my back and my neck. I have arterial hypertension but I don’t take my medication every day.'\n",
    "\n",
    "test_sentence = 'In two of the three deaths probably associated with ketoconazole treatment the drug had been continued after the onset of jaundice and other symptoms of hepatitis.'\n",
    "\n",
    "test_sentence = \"\"\" @nurse  Doctor, we have a man (51 years old) who entered the emergency department reporting chest pain. His vital signs are ABP: 144x92mmHG; HR: 78bpm; RR: 21rpm; Temp: 37oC; O2Sat: 98%.\n",
    "\n",
    "@patient  Doctor, I am feeling chest pain since yesterday. The pain is continuous and is located just in the middle of my chest, worsening when I breathe and when I lay down on my bed. I suffer from arterial hypertension and smoke 20 cigarettes every day. My father had a “heart attack” at my age and I am very worried about it.\n",
    "I use sertralina.\n",
    "PHYSICAL EXAMINATION\n",
    "\n",
    "The cardiac and pulmonary auscultation are normal; chest pain does not worse with palpation of the thorax; there is no jugular stasis nor lower limb edema.\n",
    "\n",
    "@system  What do you want to do?\n",
    "\n",
    "    Generate hypothesis\n",
    "    More information\n",
    "    Call the supervisor \"\"\"\n",
    "\n",
    "\n",
    "# print(test_sentence)\n",
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "# print(tokenized_sentence)\n",
    "# input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "input_ids = torch.tensor([tokenized_sentence])\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "# print(output)\n",
    "\n",
    "\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "# print(label_indices)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "#     print(label_idx)\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "#         print('else')\n",
    "#         print(new_labels)\n",
    "#         print(new_tokens)\n",
    "        new_labels.append(labels[label_idx])\n",
    "        new_tokens.append(token)\n",
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t\\t{}\".format(token, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = '[CLS] '\n",
    "# test_sentence += 'In two of the three deaths probably associated with ketoconazole treatment the drug had been continued after the onset of jaundice and other symptoms of hepatitis.'\n",
    "test_sentence += 'Two patients with type II diabetes mellitus developed and acute hepatites-like syndrome soon after initiation of glyburide therapy.'\n",
    "test_sentence += '[SEP]'\n",
    "words = separar_frases(test_sentence)\n",
    "# tokenize(words)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word in sentence:\n",
    "#         print('sentence ', word)\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "#         print(word)\n",
    "#         break\n",
    "        tokenized_word = tokenizer.tokenize(str(word))\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "#         # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "#         # Add the same label to the new list of labels `n_subwords` times\n",
    "#         if label.startswith(\"B\"):\n",
    "#             labels.extend([label])\n",
    "#             new_label = \"I-\" + label[2:]\n",
    "\n",
    "#             labels.extend([new_label] * (n_subwords-1))\n",
    "#         else:\n",
    "#             labels.extend([label] * n_subwords)\n",
    "\n",
    "\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-reply",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-marriage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
