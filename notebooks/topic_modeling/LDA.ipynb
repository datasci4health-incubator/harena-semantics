{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88637ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "DATASET = 'clicr'\n",
    "\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "FILE = '../datasets/' + DATASET + '/sequences.txt'\n",
    "\n",
    "with open(FILE) as f:\n",
    "    data_samples = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8ccf6",
   "metadata": {},
   "source": [
    "## Gemsim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07770730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary, common_texts, simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# def preprocess(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# print(data_samples[0])\n",
    "data_samples2 = data_samples\n",
    "\n",
    "# [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_samples2]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# data_samples2 = remove_stopwords(data_samples2)\n",
    "# print(data_samples2)\n",
    "# print(data_samples2)\n",
    "\n",
    "# lista = []\n",
    "# for sentence in data_samples2:\n",
    "# #     print()\n",
    "#     aqui = simple_preprocess(str(sentence), deacc=True)\n",
    "# #     print(aqui)\n",
    "\n",
    "\n",
    "# data_samples = preprocess(data_samples2)\n",
    "\n",
    "# print('lista, :', lista)\n",
    "\n",
    "for idx in range(len(data_samples2)):\n",
    "#     print(data_samples2[idx])\n",
    "    data_samples2[idx] = data_samples2[idx].lower()  # Convert to lowercase.\n",
    "    data_samples2[idx] = tokenizer.tokenize(data_samples2[idx])  # Split into words.\n",
    "\n",
    "# print(data_samples2[0])\n",
    "\n",
    "# # Remove numbers, but not words that contain numbers.\n",
    "data_samples2 = [[token for token in doc if not token.isnumeric()] for doc in data_samples2]\n",
    "data_samples2 = [[token for token in doc if token not in stop_words] for doc in data_samples2]\n",
    "\n",
    "# # Remove words that are only one character.\n",
    "data_samples2 = [[token for token in doc if len(token) > 1] for doc in data_samples2]\n",
    "\n",
    "# print(data_samples2)\n",
    "\n",
    "\n",
    "dictionary = Dictionary(data_samples2)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_samples2]\n",
    "texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "\n",
    "# print(dictionary)\n",
    "# print(corpus)\n",
    "# print(data_samples2[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ade6e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coerência: 0.7191799757808793\n",
      "Coerência por Tópico:  [0.7621357147647714, 0.5744807091889002, 0.6713446163331775, 0.6420527421315805, 0.8542059649113769, 0.7205136470540203, 0.6158012489453467, 0.8424338766312275, 0.7896512620675132]\n"
     ]
    }
   ],
   "source": [
    "# print(dictionary)\n",
    "model = LdaModel(corpus, 9, dictionary)\n",
    "\n",
    "cm = CoherenceModel(model=model, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print('Coerência:',coherence)\n",
    "\n",
    "# print(model.print_topics())\n",
    "doc_lda = model[corpus]\n",
    "# print(str(doc_lda))\n",
    "\n",
    "# coherence_per_topic = cm.get_coherence_per_topic()\n",
    "# coherence_per_topic.sort()\n",
    "print('Coerência por Tópico: ',cm.get_coherence_per_topic())\n",
    "# model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1405b85-e141-4895-9b24-7c6c25fd20dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coerência por Tópico:  [0.7621357147647714, 0.5744807091889002, 0.6713446163331775, 0.6420527421315805, 0.8542059649113769, 0.7205136470540203, 0.6158012489453467, 0.8424338766312275, 0.7896512620675132]\n"
     ]
    }
   ],
   "source": [
    "# Ordenar\n",
    "# coherence_per_topic = cm.get_coherence_per_topic()\n",
    "cm.get_coherence_per_topic().sort()\n",
    "print('Coerência por Tópico: ',cm.get_coherence_per_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f51036c4-3c56-4daf-bd8e-f2bb6bd2215a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tumour', 0.011366457),\n",
       " ('patient', 0.00947242),\n",
       " ('case', 0.0070538786),\n",
       " ('treatment', 0.0059954505),\n",
       " ('cancer', 0.0055896738),\n",
       " ('patients', 0.005231302),\n",
       " ('tumours', 0.0049823592),\n",
       " ('ct', 0.0047540152),\n",
       " ('right', 0.0046980507),\n",
       " ('figure', 0.004651175),\n",
       " ('mass', 0.0046202606),\n",
       " ('carcinoma', 0.0044094916),\n",
       " ('left', 0.004256249),\n",
       " ('disease', 0.0041787685),\n",
       " ('breast', 0.0039463835)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topic(6, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56954571-a978-45f2-9f17-e21c1849cc04",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 168 is out of bounds for axis 0 with size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topic_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/models/ldamodel.py:1248\u001b[0m, in \u001b[0;36mLdaModel.get_topic_terms\u001b[0;34m(self, topicid, topn)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_terms\u001b[39m(\u001b[38;5;28mself\u001b[39m, topicid, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the representation for a single topic. Words the integer IDs, in constrast to\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1248\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtopicid\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1249\u001b[0m     topic \u001b[38;5;241m=\u001b[39m topic \u001b[38;5;241m/\u001b[39m topic\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# normalize to probability distribution\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     bestn \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39margsort(topic, topn, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 168 is out of bounds for axis 0 with size 9"
     ]
    }
   ],
   "source": [
    "model.get_topic_terms(168, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c189fa5-b757-4d2d-8339-b6ade6f1c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.top_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f178971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8491640",
   "metadata": {},
   "source": [
    "## SciKit LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69fbdf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for LDA...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(data_samples[0])\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone in \u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time() \u001b[38;5;241m-\u001b[39m t0))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Use tf (raw term count) features for LDA.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m \n\u001b[1;32m   2060\u001b[0m \u001b[38;5;124;03mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m-> 2077\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1327\u001b[0m             )\n\u001b[1;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from time import time\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "\n",
    "# with open(FILE) as f:\n",
    "#     data_samples = f.readlines()\n",
    "\n",
    "#     print(data_samples)\n",
    "    \n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for LDA...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "# print(data_samples[0])\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "\n",
    "aux_lda = lda.fit(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in scikit LDA model\")\n",
    "\n",
    "print('LDA perplexity:', aux_lda.perplexity(tfidf[0]))\n",
    "# perplexity.append(news_lda.perplexity(doc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ea2d3",
   "metadata": {},
   "source": [
    "# ---------------------\n",
    "## Exemplo tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763744aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "#     print(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(topic)\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "#         print(top_features_ind)\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "#         print(top_features)\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "data, y = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "data_samples = data\n",
    "# data_samples = data[:n_samples]\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "# print(type(tfidf))\n",
    "# print(tfidf.shape)\n",
    "# print(len(tfidf[0]))\n",
    "\n",
    "# print(tfidf.toarray())\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bcd33",
   "metadata": {},
   "source": [
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "# This produces a feature matrix of token counts, similar to what CountVectorizer would produce on text.\n",
    "X, Y = make_multilabel_classification(random_state=0)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# get topics for some given samples:\n",
    "print(lda.transform(X[-2:]))\n",
    "print('Perplexity ', lda.perplexity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "texts = ['eu voce que tal', 'ola meu bem', 'bora?']\n",
    "fitted = vec.fit_transform(texts)\n",
    "fitted.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d376967",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "topics = [\n",
    "\n",
    "    ['human', 'computer', 'system', 'interface'],\n",
    "\n",
    "    ['graph', 'minors', 'trees', 'eps']\n",
    "\n",
    "]\n",
    "\n",
    "cm = CoherenceModel(topics=topics, corpus=common_corpus, dictionary=common_dictionary, coherence='u_mass')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print(coherence)\n",
    "print(cm.get_coherence_per_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "texts2 = [['eu voce que tal', 'ola meu bem', 'bora?']]\n",
    "\n",
    "\n",
    "dct = Dictionary(texts2)\n",
    "bow = dct.doc2bow([\"eu voce que tal\", \"ola meu bem\"])\n",
    "# bow = dct.doc2bow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33606cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct2 = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
    "str(dct2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa88971",
   "metadata": {},
   "source": [
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import CoherenceModel, LdaModel, HdpModel\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# # texts = [['human', 'interface', 'computer'],\n",
    "# #          ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "# #          ['eps', 'user', 'interface', 'system'],\n",
    "# #          ['system', 'human', 'system', 'eps'],\n",
    "# #          ['user', 'response', 'time'],\n",
    "# #          ['trees'],\n",
    "# #          ['graph', 'trees'],\n",
    "# #          ['graph', 'minors', 'trees'],\n",
    "# #          ['graph', 'minors', 'survey']]\n",
    "# # texts = [['human', 'interface', 'computer'], ['coronary', 'ecg']]\n",
    "# texts = data_samples\n",
    "# dictionary = Dictionary(texts)\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# print(corpus)\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "# import numpy as np\n",
    "# # lda_model - LatentDirichletAllocation()\n",
    "# # vect - CountVectorizer()\n",
    "# # texts - the list of tokenized words\n",
    "# metric_coherence_gensim(measure='c_v', \n",
    "#                         top_n=25, \n",
    "#                         topic_word_distrib=lda.components_, \n",
    "# #                         dtm=dtm_tf, \n",
    "#                         gensim_corpus  = corpus,                     \n",
    "#                         vocab=np.array([x for x in vect.vocabulary_.keys()]), \n",
    "#                         texts=train['cleaned_NOUN'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde62c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655bea7-227f-4ea3-b8c0-29809e6853cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb11dd1-eae8-4ba1-aba0-051f472fcc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
