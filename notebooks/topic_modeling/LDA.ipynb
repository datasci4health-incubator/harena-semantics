{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88637ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "DATASET = 'clicr'\n",
    "\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "FILE = '../datasets/' + DATASET + '/sequences.txt'\n",
    "\n",
    "OUTPUT_PATH = 'output/'\n",
    "\n",
    "with open(FILE) as f:\n",
    "    data_samples = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8ccf6",
   "metadata": {},
   "source": [
    "## Gemsim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07770730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary, common_texts, simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# def preprocess(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# print(data_samples[0])\n",
    "data_samples2 = data_samples\n",
    "\n",
    "# [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_samples2]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# data_samples2 = remove_stopwords(data_samples2)\n",
    "# print(data_samples2)\n",
    "# print(data_samples2)\n",
    "\n",
    "# lista = []\n",
    "# for sentence in data_samples2:\n",
    "# #     print()\n",
    "#     aqui = simple_preprocess(str(sentence), deacc=True)\n",
    "# #     print(aqui)\n",
    "\n",
    "\n",
    "# data_samples = preprocess(data_samples2)\n",
    "\n",
    "# print('lista, :', lista)\n",
    "\n",
    "for idx in range(len(data_samples2)):\n",
    "#     print(data_samples2[idx])\n",
    "    data_samples2[idx] = data_samples2[idx].lower()  # Convert to lowercase.\n",
    "    data_samples2[idx] = tokenizer.tokenize(data_samples2[idx])  # Split into words.\n",
    "\n",
    "# print(data_samples2[0])\n",
    "\n",
    "# # Remove numbers, but not words that contain numbers.\n",
    "data_samples2 = [[token for token in doc if not token.isnumeric()] for doc in data_samples2]\n",
    "data_samples2 = [[token for token in doc if token not in stop_words] for doc in data_samples2]\n",
    "\n",
    "# # Remove words that are only one character.\n",
    "data_samples2 = [[token for token in doc if len(token) > 1] for doc in data_samples2]\n",
    "\n",
    "# print(data_samples2)\n",
    "\n",
    "\n",
    "dictionary = Dictionary(data_samples2)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_samples2]\n",
    "texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "\n",
    "# print(dictionary)\n",
    "# print(corpus)\n",
    "# print(data_samples2[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dictionary)\n",
    "N_TOPICS = 7\n",
    "model = LdaModel(corpus, N_TOPICS, dictionary)\n",
    "\n",
    "cm = CoherenceModel(model=model, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v', topn=50)\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print('Coerência:',coherence)\n",
    "\n",
    "# print(model.print_topics())\n",
    "doc_lda = model[corpus]\n",
    "# print(str(doc_lda))\n",
    "\n",
    "# coherence_per_topic = cm.get_coherence_per_topic()\n",
    "# coherence_per_topic.sort()\n",
    "print('Coerência por Tópico: ',cm.get_coherence_per_topic())\n",
    "\n",
    "with open(OUTPUT_PATH + 'notes.txt', \"w\") as file:\n",
    "    # print('Limiar de distância: '+str(distance_threshold), file=file)   \n",
    "    print('Qtdd de Tópicos: '+str(N_TOPICS), file=file)   \n",
    "    # print('Limiar TF-IDF: '+str(MAX_DF), file=file)  \n",
    "    print('Coerência total: '+str(coherence), file=file)  \n",
    "    print('Coerência por Tópicos: '+str(cm.get_coherence_per_topic()), file=file)  \n",
    "    # print('Top Words: '+str(TOP_WORDS), file=file)  \n",
    "\n",
    "    print('----------------------------------------------------------------------------', file=file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1405b85-e141-4895-9b24-7c6c25fd20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar\n",
    "# coherence_per_topic = cm.get_coherence_per_topic()\n",
    "cm.get_coherence_per_topic().sort()\n",
    "print('Coerência por Tópico: ',cm.get_coherence_per_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51036c4-3c56-4daf-bd8e-f2bb6bd2215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic(6, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56954571-a978-45f2-9f17-e21c1849cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_terms(168, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c189fa5-b757-4d2d-8339-b6ade6f1c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.top_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f178971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8491640",
   "metadata": {},
   "source": [
    "## SciKit LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbdf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from time import time\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "\n",
    "# with open(FILE) as f:\n",
    "#     data_samples = f.readlines()\n",
    "\n",
    "#     print(data_samples)\n",
    "    \n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for LDA...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "# print(data_samples[0])\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "\n",
    "aux_lda = lda.fit(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in scikit LDA model\")\n",
    "\n",
    "print('LDA perplexity:', aux_lda.perplexity(tfidf[0]))\n",
    "# perplexity.append(news_lda.perplexity(doc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ea2d3",
   "metadata": {},
   "source": [
    "# ---------------------\n",
    "## Exemplo tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763744aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "#     print(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(topic)\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "#         print(top_features_ind)\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "#         print(top_features)\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "data, y = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "data_samples = data\n",
    "# data_samples = data[:n_samples]\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "# print(type(tfidf))\n",
    "# print(tfidf.shape)\n",
    "# print(len(tfidf[0]))\n",
    "\n",
    "# print(tfidf.toarray())\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bcd33",
   "metadata": {},
   "source": [
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "# This produces a feature matrix of token counts, similar to what CountVectorizer would produce on text.\n",
    "X, Y = make_multilabel_classification(random_state=0)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# get topics for some given samples:\n",
    "print(lda.transform(X[-2:]))\n",
    "print('Perplexity ', lda.perplexity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "texts = ['eu voce que tal', 'ola meu bem', 'bora?']\n",
    "fitted = vec.fit_transform(texts)\n",
    "fitted.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d376967",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "topics = [\n",
    "\n",
    "    ['human', 'computer', 'system', 'interface'],\n",
    "\n",
    "    ['graph', 'minors', 'trees', 'eps']\n",
    "\n",
    "]\n",
    "\n",
    "cm = CoherenceModel(topics=topics, corpus=common_corpus, dictionary=common_dictionary, coherence='u_mass')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print(coherence)\n",
    "print(cm.get_coherence_per_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "texts2 = [['eu voce que tal', 'ola meu bem', 'bora?']]\n",
    "\n",
    "\n",
    "dct = Dictionary(texts2)\n",
    "bow = dct.doc2bow([\"eu voce que tal\", \"ola meu bem\"])\n",
    "# bow = dct.doc2bow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33606cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct2 = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
    "str(dct2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa88971",
   "metadata": {},
   "source": [
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import CoherenceModel, LdaModel, HdpModel\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# # texts = [['human', 'interface', 'computer'],\n",
    "# #          ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "# #          ['eps', 'user', 'interface', 'system'],\n",
    "# #          ['system', 'human', 'system', 'eps'],\n",
    "# #          ['user', 'response', 'time'],\n",
    "# #          ['trees'],\n",
    "# #          ['graph', 'trees'],\n",
    "# #          ['graph', 'minors', 'trees'],\n",
    "# #          ['graph', 'minors', 'survey']]\n",
    "# # texts = [['human', 'interface', 'computer'], ['coronary', 'ecg']]\n",
    "# texts = data_samples\n",
    "# dictionary = Dictionary(texts)\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# print(corpus)\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "# import numpy as np\n",
    "# # lda_model - LatentDirichletAllocation()\n",
    "# # vect - CountVectorizer()\n",
    "# # texts - the list of tokenized words\n",
    "# metric_coherence_gensim(measure='c_v', \n",
    "#                         top_n=25, \n",
    "#                         topic_word_distrib=lda.components_, \n",
    "# #                         dtm=dtm_tf, \n",
    "#                         gensim_corpus  = corpus,                     \n",
    "#                         vocab=np.array([x for x in vect.vocabulary_.keys()]), \n",
    "#                         texts=train['cleaned_NOUN'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde62c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655bea7-227f-4ea3-b8c0-29809e6853cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb11dd1-eae8-4ba1-aba0-051f472fcc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
