{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88637ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "DATASET = 'clicr'\n",
    "\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "FILE = '../datasets/' + DATASET + '/sequences.txt'\n",
    "\n",
    "OUTPUT_PATH = 'output/'\n",
    "\n",
    "with open(FILE) as f:\n",
    "    data_samples = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8ccf6",
   "metadata": {},
   "source": [
    "## Gemsim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07770730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary, common_texts, simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# def preprocess(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# print(data_samples[0])\n",
    "data_samples2 = data_samples\n",
    "\n",
    "# [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_samples2]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# data_samples2 = remove_stopwords(data_samples2)\n",
    "# print(data_samples2)\n",
    "# print(data_samples2)\n",
    "\n",
    "# lista = []\n",
    "# for sentence in data_samples2:\n",
    "# #     print()\n",
    "#     aqui = simple_preprocess(str(sentence), deacc=True)\n",
    "# #     print(aqui)\n",
    "\n",
    "\n",
    "# data_samples = preprocess(data_samples2)\n",
    "\n",
    "# print('lista, :', lista)\n",
    "\n",
    "for idx in range(len(data_samples2)):\n",
    "#     print(data_samples2[idx])\n",
    "    data_samples2[idx] = data_samples2[idx].lower()  # Convert to lowercase.\n",
    "    data_samples2[idx] = tokenizer.tokenize(data_samples2[idx])  # Split into words.\n",
    "\n",
    "# print(data_samples2[0])\n",
    "\n",
    "# # Remove numbers, but not words that contain numbers.\n",
    "data_samples2 = [[token for token in doc if not token.isnumeric()] for doc in data_samples2]\n",
    "data_samples2 = [[token for token in doc if token not in stop_words] for doc in data_samples2]\n",
    "\n",
    "# # Remove words that are only one character.\n",
    "data_samples2 = [[token for token in doc if len(token) > 1] for doc in data_samples2]\n",
    "\n",
    "# print(data_samples2)\n",
    "\n",
    "\n",
    "dictionary = Dictionary(data_samples2)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_samples2]\n",
    "texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "\n",
    "# print(dictionary)\n",
    "# print(corpus)\n",
    "# print(data_samples2[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ade6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dictionary)\n",
    "N_TOPICS = 150\n",
    "model = LdaModel(corpus, N_TOPICS, dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfb9f39-1754-4730-9f3d-8055ddf76510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coerência: 0.4099853675938067\n",
      "Coerência por Tópico:  [0.3193919626533399, 0.28513100456336365, 0.4283194341924772, 0.3257389261551415, 0.4018068468179588, 0.43685186645372476, 0.4553022241458507, 0.5431267493878008, 0.3933175439807207, 0.49317404221374217, 0.4773336601352851, 0.5101315118280294, 0.49319741627755787, 0.2555832120345127, 0.4120290867278693, 0.27071363476667337, 0.40803711352346744, 0.5908303493280169, 0.49877039004605, 0.49286996898236785, 0.42248654506334676, 0.5983946579488659, 0.5405271896265961, 0.28505284455232455, 0.5516689019764516, 0.18209188031677592, 0.18787444910616544, 0.48959609679882127, 0.3383967064008071, 0.49346546324554674, 0.38895681901901236, 0.40942404079276673, 0.44764904021478974, 0.5165939488608013, 0.4205093031544889, 0.5262422755610183, 0.4964139475561944, 0.18783645809098087, 0.39171334440691274, 0.3574273644370678, 0.42148033445177957, 0.4933595336683299, 0.11849304279469917, 0.40047595076803444, 0.24529011450463517, 0.22569550149327214, 0.45868616232555376, 0.22899855710243203, 0.3615539748120262, 0.5095110753081887, 0.3858652300253352, 0.5062452904324477, 0.5074731527120577, 0.3249129051027714, 0.35100269769487324, 0.39323049637314056, 0.286237416075537, 0.39186098045753687, 0.24004587226088805, 0.361756356927258, 0.33866410952270926, 0.4428972226802581, 0.4331966963921332, 0.36524328240352844, 0.4076838371879062, 0.3202355091963618, 0.3431952650937531, 0.33204763883280153, 0.5189856741200901, 0.37628847026145934, 0.4154106092390981, 0.46188598992964225, 0.4316992092800935, 0.21070897517347917, 0.6369129179867205, 0.31341512541893357, 0.38285542139356904, 0.5588120571919779, 0.31166164140724656, 0.3763066345338196, 0.34773088817689585, 0.4778142841084433, 0.27343076957282436, 0.19309813301518616, 0.4379166478330395, 0.44976315408781586, 0.6098987096780845, 0.48130217332856545, 0.16250804502354554, 0.3952776744718093, 0.3925414754815557, 0.42873841619415065, 0.26691391885208837, 0.48753348572798544, 0.27869506913923414, 0.4667029514907587, 0.3686423260429132, 0.535628599207722, 0.5637721156049768, 0.36197324974083706, 0.39932895414274855, 0.4238187442814396, 0.3975294730681088, 0.4768326165341397, 0.5626208825336384, 0.5336233448630525, 0.5248717959086935, 0.5060698610788398, 0.2993643136243054, 0.5028793471539411, 0.3296633658716121, 0.44063642557015065, 0.5425479651076557, 0.32273378312382667, 0.4160466182676254, 0.38769780429717005, 0.6087672908396263, 0.5120061251901394, 0.5026580993379054, 0.5077194611768243, 0.41986351885472545, 0.38377728513491155, 0.3999250954105162, 0.5738474114246374, 0.5285433320912696, 0.3678944127722218, 0.4386575950281525, 0.62100828465648, 0.5122817691801069, 0.39869337414569955, 0.3483571470454607, 0.4837017752237201, 0.46998681611037646, 0.29431501963196494, 0.15025978389378566, 0.5178125785759257, 0.3537842181529374, 0.4462985597070768, 0.486817772950401, 0.2739713978914187, 0.4590242061564498, 0.4362854816094765, 0.4719791165591642, 0.23325806518577427, 0.19913778394582823, 0.4433826228784951, 0.36852407079812616, 0.4409304412301248, 0.45364640131041967, 0.4302822888854142]\n"
     ]
    }
   ],
   "source": [
    "TOP_WORDS = 50\n",
    "\n",
    "cm = CoherenceModel(model=model, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v', topn=TOP_WORDS)\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print('Coerência:',coherence)\n",
    "\n",
    "# print(model.print_topics())\n",
    "doc_lda = model[corpus]\n",
    "# print(str(doc_lda))\n",
    "\n",
    "# coherence_per_topic = cm.get_coherence_per_topic()\n",
    "# coherence_per_topic.sort()\n",
    "print('Coerência por Tópico: ',cm.get_coherence_per_topic())\n",
    "\n",
    "with open(OUTPUT_PATH + 'notes.txt', \"w\") as file:\n",
    "    # print('Limiar de distância: '+str(distance_threshold), file=file)   \n",
    "    print('Qtdd de Tópicos: '+str(N_TOPICS), file=file)   \n",
    "    # print('Limiar TF-IDF: '+str(MAX_DF), file=file)  \n",
    "    print('Coerência total: '+str(coherence), file=file)  \n",
    "    print('Coerência por Tópicos: '+str(cm.get_coherence_per_topic()), file=file)  \n",
    "    # print('Top Words: '+str(TOP_WORDS), file=file)  \n",
    "\n",
    "    print('----------------------------------------------------------------------------', file=file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1405b85-e141-4895-9b24-7c6c25fd20dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coerência por Tópico:  [0.3193919626533399, 0.28513100456336365, 0.4283194341924772, 0.3257389261551415, 0.4018068468179588, 0.43685186645372476, 0.4553022241458507, 0.5431267493878008, 0.3933175439807207, 0.49317404221374217, 0.4773336601352851, 0.5101315118280294, 0.49319741627755787, 0.2555832120345127, 0.4120290867278693, 0.27071363476667337, 0.40803711352346744, 0.5908303493280169, 0.49877039004605, 0.49286996898236785, 0.42248654506334676, 0.5983946579488659, 0.5405271896265961, 0.28505284455232455, 0.5516689019764516, 0.18209188031677592, 0.18787444910616544, 0.48959609679882127, 0.3383967064008071, 0.49346546324554674, 0.38895681901901236, 0.40942404079276673, 0.44764904021478974, 0.5165939488608013, 0.4205093031544889, 0.5262422755610183, 0.4964139475561944, 0.18783645809098087, 0.39171334440691274, 0.3574273644370678, 0.42148033445177957, 0.4933595336683299, 0.11849304279469917, 0.40047595076803444, 0.24529011450463517, 0.22569550149327214, 0.45868616232555376, 0.22899855710243203, 0.3615539748120262, 0.5095110753081887, 0.3858652300253352, 0.5062452904324477, 0.5074731527120577, 0.3249129051027714, 0.35100269769487324, 0.39323049637314056, 0.286237416075537, 0.39186098045753687, 0.24004587226088805, 0.361756356927258, 0.33866410952270926, 0.4428972226802581, 0.4331966963921332, 0.36524328240352844, 0.4076838371879062, 0.3202355091963618, 0.3431952650937531, 0.33204763883280153, 0.5189856741200901, 0.37628847026145934, 0.4154106092390981, 0.46188598992964225, 0.4316992092800935, 0.21070897517347917, 0.6369129179867205, 0.31341512541893357, 0.38285542139356904, 0.5588120571919779, 0.31166164140724656, 0.3763066345338196, 0.34773088817689585, 0.4778142841084433, 0.27343076957282436, 0.19309813301518616, 0.4379166478330395, 0.44976315408781586, 0.6098987096780845, 0.48130217332856545, 0.16250804502354554, 0.3952776744718093, 0.3925414754815557, 0.42873841619415065, 0.26691391885208837, 0.48753348572798544, 0.27869506913923414, 0.4667029514907587, 0.3686423260429132, 0.535628599207722, 0.5637721156049768, 0.36197324974083706, 0.39932895414274855, 0.4238187442814396, 0.3975294730681088, 0.4768326165341397, 0.5626208825336384, 0.5336233448630525, 0.5248717959086935, 0.5060698610788398, 0.2993643136243054, 0.5028793471539411, 0.3296633658716121, 0.44063642557015065, 0.5425479651076557, 0.32273378312382667, 0.4160466182676254, 0.38769780429717005, 0.6087672908396263, 0.5120061251901394, 0.5026580993379054, 0.5077194611768243, 0.41986351885472545, 0.38377728513491155, 0.3999250954105162, 0.5738474114246374, 0.5285433320912696, 0.3678944127722218, 0.4386575950281525, 0.62100828465648, 0.5122817691801069, 0.39869337414569955, 0.3483571470454607, 0.4837017752237201, 0.46998681611037646, 0.29431501963196494, 0.15025978389378566, 0.5178125785759257, 0.3537842181529374, 0.4462985597070768, 0.486817772950401, 0.2739713978914187, 0.4590242061564498, 0.4362854816094765, 0.4719791165591642, 0.23325806518577427, 0.19913778394582823, 0.4433826228784951, 0.36852407079812616, 0.4409304412301248, 0.45364640131041967, 0.4302822888854142]\n"
     ]
    }
   ],
   "source": [
    "# Ordenar\n",
    "# coherence_per_topic = cm.get_coherence_per_topic()\n",
    "cm.get_coherence_per_topic().sort()\n",
    "print('Coerência por Tópico: ',cm.get_coherence_per_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f51036c4-3c56-4daf-bd8e-f2bb6bd2215a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vt', 0.012639127),\n",
       " ('patient', 0.009063685),\n",
       " ('mucosa', 0.008762592),\n",
       " ('case', 0.007022371),\n",
       " ('treatment', 0.0065439427),\n",
       " ('pigmented', 0.00635571),\n",
       " ('sap', 0.0063416935),\n",
       " ('disease', 0.0062433374),\n",
       " ('black', 0.005916713),\n",
       " ('deposition', 0.005518202),\n",
       " ('cells', 0.0054003745),\n",
       " ('collagenous', 0.005337491),\n",
       " ('subepithelial', 0.004882326),\n",
       " ('figure', 0.0048335968),\n",
       " ('cell', 0.0042852107)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topic(6, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56954571-a978-45f2-9f17-e21c1849cc04",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 168 is out of bounds for axis 0 with size 150",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topic_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/models/ldamodel.py:1248\u001b[0m, in \u001b[0;36mLdaModel.get_topic_terms\u001b[0;34m(self, topicid, topn)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_terms\u001b[39m(\u001b[38;5;28mself\u001b[39m, topicid, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the representation for a single topic. Words the integer IDs, in constrast to\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1248\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtopicid\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1249\u001b[0m     topic \u001b[38;5;241m=\u001b[39m topic \u001b[38;5;241m/\u001b[39m topic\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# normalize to probability distribution\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     bestn \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39margsort(topic, topn, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 168 is out of bounds for axis 0 with size 150"
     ]
    }
   ],
   "source": [
    "model.get_topic_terms(168, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c189fa5-b757-4d2d-8339-b6ade6f1c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.top_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f178971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8491640",
   "metadata": {},
   "source": [
    "## SciKit LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbdf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from time import time\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "\n",
    "# with open(FILE) as f:\n",
    "#     data_samples = f.readlines()\n",
    "\n",
    "#     print(data_samples)\n",
    "    \n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for LDA...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "# print(data_samples[0])\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "\n",
    "aux_lda = lda.fit(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in scikit LDA model\")\n",
    "\n",
    "print('LDA perplexity:', aux_lda.perplexity(tfidf[0]))\n",
    "# perplexity.append(news_lda.perplexity(doc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ea2d3",
   "metadata": {},
   "source": [
    "# ---------------------\n",
    "## Exemplo tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763744aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "#     print(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(topic)\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "#         print(top_features_ind)\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "#         print(top_features)\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "data, y = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "data_samples = data\n",
    "# data_samples = data[:n_samples]\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "# print(type(tfidf))\n",
    "# print(tfidf.shape)\n",
    "# print(len(tfidf[0]))\n",
    "\n",
    "# print(tfidf.toarray())\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bcd33",
   "metadata": {},
   "source": [
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "# This produces a feature matrix of token counts, similar to what CountVectorizer would produce on text.\n",
    "X, Y = make_multilabel_classification(random_state=0)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# get topics for some given samples:\n",
    "print(lda.transform(X[-2:]))\n",
    "print('Perplexity ', lda.perplexity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "texts = ['eu voce que tal', 'ola meu bem', 'bora?']\n",
    "fitted = vec.fit_transform(texts)\n",
    "fitted.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d376967",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "topics = [\n",
    "\n",
    "    ['human', 'computer', 'system', 'interface'],\n",
    "\n",
    "    ['graph', 'minors', 'trees', 'eps']\n",
    "\n",
    "]\n",
    "\n",
    "cm = CoherenceModel(topics=topics, corpus=common_corpus, dictionary=common_dictionary, coherence='u_mass')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print(coherence)\n",
    "print(cm.get_coherence_per_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "texts2 = [['eu voce que tal', 'ola meu bem', 'bora?']]\n",
    "\n",
    "\n",
    "dct = Dictionary(texts2)\n",
    "bow = dct.doc2bow([\"eu voce que tal\", \"ola meu bem\"])\n",
    "# bow = dct.doc2bow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33606cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct2 = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
    "str(dct2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa88971",
   "metadata": {},
   "source": [
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334c8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import CoherenceModel, LdaModel, HdpModel\n",
    "# from gensim.corpora import Dictionary\n",
    "\n",
    "# # texts = [['human', 'interface', 'computer'],\n",
    "# #          ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "# #          ['eps', 'user', 'interface', 'system'],\n",
    "# #          ['system', 'human', 'system', 'eps'],\n",
    "# #          ['user', 'response', 'time'],\n",
    "# #          ['trees'],\n",
    "# #          ['graph', 'trees'],\n",
    "# #          ['graph', 'minors', 'trees'],\n",
    "# #          ['graph', 'minors', 'survey']]\n",
    "# # texts = [['human', 'interface', 'computer'], ['coronary', 'ecg']]\n",
    "# texts = data_samples\n",
    "# dictionary = Dictionary(texts)\n",
    "# corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# print(corpus)\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd1da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "# import numpy as np\n",
    "# # lda_model - LatentDirichletAllocation()\n",
    "# # vect - CountVectorizer()\n",
    "# # texts - the list of tokenized words\n",
    "# metric_coherence_gensim(measure='c_v', \n",
    "#                         top_n=25, \n",
    "#                         topic_word_distrib=lda.components_, \n",
    "# #                         dtm=dtm_tf, \n",
    "#                         gensim_corpus  = corpus,                     \n",
    "#                         vocab=np.array([x for x in vect.vocabulary_.keys()]), \n",
    "#                         texts=train['cleaned_NOUN'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde62c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655bea7-227f-4ea3-b8c0-29809e6853cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb11dd1-eae8-4ba1-aba0-051f472fcc04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
