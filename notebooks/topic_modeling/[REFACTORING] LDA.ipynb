{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88637ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "DATASET = 'clicr'\n",
    "\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "FILE = '../datasets/' + DATASET + '/sequences.txt'\n",
    "\n",
    "OUTPUT_PATH = 'output/'\n",
    "\n",
    "with open(FILE) as f:\n",
    "    data_samples = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8ccf6",
   "metadata": {},
   "source": [
    "## Gemsim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07770730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary, common_texts, simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# def preprocess(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# print(data_samples[0])\n",
    "data_samples2 = data_samples\n",
    "\n",
    "# [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_samples2]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# data_samples2 = remove_stopwords(data_samples2)\n",
    "# print(data_samples2)\n",
    "# print(data_samples2)\n",
    "\n",
    "# lista = []\n",
    "# for sentence in data_samples2:\n",
    "# #     print()\n",
    "#     aqui = simple_preprocess(str(sentence), deacc=True)\n",
    "# #     print(aqui)\n",
    "\n",
    "\n",
    "# data_samples = preprocess(data_samples2)\n",
    "\n",
    "# print('lista, :', lista)\n",
    "\n",
    "for idx in range(len(data_samples2)):\n",
    "#     print(data_samples2[idx])\n",
    "    data_samples2[idx] = data_samples2[idx].lower()  # Convert to lowercase.\n",
    "    data_samples2[idx] = tokenizer.tokenize(data_samples2[idx])  # Split into words.\n",
    "\n",
    "# print(data_samples2[0])\n",
    "\n",
    "# # Remove numbers, but not words that contain numbers.\n",
    "data_samples2 = [[token for token in doc if not token.isnumeric()] for doc in data_samples2]\n",
    "data_samples2 = [[token for token in doc if token not in stop_words] for doc in data_samples2]\n",
    "\n",
    "# # Remove words that are only one character.\n",
    "data_samples2 = [[token for token in doc if len(token) > 1] for doc in data_samples2]\n",
    "\n",
    "# print(data_samples2)\n",
    "\n",
    "\n",
    "dictionary = Dictionary(data_samples2)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_samples2]\n",
    "texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "\n",
    "# print(dictionary)\n",
    "# print(corpus)\n",
    "# print(data_samples2[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ade6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dictionary)\n",
    "N_TOPICS = 150\n",
    "model = LdaModel(corpus, N_TOPICS, dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1405b85-e141-4895-9b24-7c6c25fd20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_WORDS = 80\n",
    "cm = CoherenceModel(model=model, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v', topn=TOP_WORDS)\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "\n",
    "# print(str(doc_lda))\n",
    "\n",
    "print('Coerência:',coherence)\n",
    "print('Coerência por Tópico: ',coherence_per_topic)\n",
    "\n",
    "with open(OUTPUT_PATH + 'notes.txt', \"w\") as file:\n",
    "    # print('Limiar de distância: '+str(distance_threshold), file=file)   \n",
    "    print('Qtdd de Tópicos: '+str(N_TOPICS), file=file)   \n",
    "    # print('Limiar TF-IDF: '+str(MAX_DF), file=file)  \n",
    "    print('Coerência total: '+str(coherence), file=file)  \n",
    "    print('Coerência por Tópicos: '+str(coherence_per_topic), file=file)  \n",
    "    print('Top Words: '+str(TOP_WORDS), file=file)  \n",
    "\n",
    "    print('----------------------------------------------------------------------------', file=file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f51036c4-3c56-4daf-bd8e-f2bb6bd2215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coerência por Tópico ordenado:  [0.35341503214488224, 0.19546109828234529, 0.4380748891309948, 0.49084099559455907, 0.4573975131137634, 0.23149337475239617, 0.3865133298011537, 0.43339550271709903, 0.2557578673272014, 0.5406719271318032, 0.45117278653951204, 0.17456867062047093, 0.43316407067550233, 0.29612611093508834, 0.49204471776896674, 0.5535512367694685, 0.38468799652139196, 0.24655214223307195, 0.25754562487804583, 0.3284062118090696, 0.37465646660401175, 0.4107440979518512, 0.33545913511790715, 0.4387887456807378, 0.5333850913106647, 0.5891897085505198, 0.2887685366491226, 0.31321117599568893, 0.18099740312870943, 0.34630497869270765, 0.517809940534214, 0.22923919600372217, 0.6688654109320084, 0.345901597958565, 0.3371009082784887, 0.4873063088167472, 0.3325332426228255, 0.35054657023549496, 0.4336622604089467, 0.44634317360318915, 0.4223170358497309, 0.2726407777176054, 0.6073694290660143, 0.4232577131551021, 0.5475793599702329, 0.6293560311704854, 0.34145200313995094, 0.20514113229012207, 0.4253356919652238, 0.2932436206383294, 0.580737525485443, 0.41607290050442, 0.28905421558647043, 0.3706870920609644, 0.3761137470739171, 0.6849806222971143, 0.2613903807820433, 0.3647166311564335, 0.41209958072140074, 0.4815719102203138, 0.43698589854316483, 0.37625384360239295, 0.37327321682643927, 0.3024134710468645, 0.2741170898665869, 0.3033030480228624, 0.4486308691300865, 0.4936398564058487, 0.16386851160264643, 0.40190591811626913, 0.3802981756633804, 0.24416498802547393, 0.29919083855991013, 0.559836793179909, 0.36476900935086876, 0.39803230775876786, 0.41940271689554864, 0.5876180046188579, 0.23158839390357064, 0.2795182863013453, 0.3914234167553591, 0.4183224391113022, 0.21553224140769114, 0.4504014480848775, 0.6256344262411593, 0.3323962922732229, 0.3362033555936237, 0.22064697269501948, 0.43322310811509357, 0.5011611721990004, 0.4718347457233628, 0.3526600101701877, 0.6269527894392296, 0.1823695745407791, 0.35578813106140667, 0.38570166073931617, 0.3568055233923783, 0.5444129602425816, 0.40552184021463333, 0.504889146977287, 0.4121002925702374, 0.5093240350458494, 0.3584382798686033, 0.31264791829593813, 0.41717160461224434, 0.4900351699762437, 0.27559863127690165, 0.3445067354750533, 0.35337585458756465, 0.15568171589823732, 0.4159139648913115, 0.45164425223360655, 0.3598482032670958, 0.4503773074188405, 0.40732117000725926, 0.38847491821904756, 0.4719659403367933, 0.4651675635511091, 0.38012794642111924, 0.37806295522556155, 0.6882001830478759, 0.306972473999041, 0.27265727697765657, 0.3960533080308584, 0.8326861922315926, 0.48204823839261557, 0.44587978045435694, 0.15068036591956147, 0.503524725326507, 0.2776074185054891, 0.3242763581427957, 0.18701909980562875, 0.4370983945710595, 0.4104683442245879, 0.45109280393023543, 0.3282949873566284, 0.3779755504042164, 0.4519038703858563, 0.37402210783263246, 0.3929923882591061, 0.32493048360245674, 0.4570438381583736, 0.2777962712463516, 0.33037770127543953, 0.5749918864573125, 0.458316371148381, 0.3953090198719128, 0.4006840357318914, 0.39907743644283505, 0.510868186294519]\n"
     ]
    }
   ],
   "source": [
    "cm.get_coherence_per_topic().sort()\n",
    "print('Coerência por Tópico ordenado: ',cm.get_coherence_per_topic())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56954571-a978-45f2-9f17-e21c1849cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(116, '0.061*\"pleural\" + 0.042*\"psoas\" + 0.034*\"bcg\" + 0.026*\"instillation\" + 0.021*\"intravesical\" + 0.020*\"gb\" + 0.015*\"effusion\" + 0.013*\"thoracentesis\" + 0.013*\"osa\" + 0.013*\"mmc\"'), (111, '0.013*\"band\" + 0.009*\"may\" + 0.008*\"case\" + 0.008*\"albendazole\" + 0.007*\"tmj\" + 0.007*\"compacted\" + 0.006*\"left\" + 0.006*\"right\" + 0.006*\"diagnosis\" + 0.006*\"patient\"'), (42, '0.014*\"renal\" + 0.007*\"right\" + 0.007*\"patient\" + 0.007*\"diagnosis\" + 0.006*\"showed\" + 0.005*\"left\" + 0.005*\"case\" + 0.005*\"normal\" + 0.004*\"patients\" + 0.004*\"figure\"'), (75, '0.011*\"igg4\" + 0.008*\"nerve\" + 0.008*\"case\" + 0.008*\"cystic\" + 0.007*\"patient\" + 0.006*\"cyst\" + 0.005*\"figure\" + 0.005*\"diagnosis\" + 0.005*\"cysts\" + 0.005*\"chronic\"'), (43, '0.013*\"diagnosis\" + 0.011*\"tb\" + 0.009*\"case\" + 0.008*\"cases\" + 0.007*\"patient\" + 0.007*\"mass\" + 0.007*\"lesion\" + 0.006*\"tuberculosis\" + 0.006*\"biopsy\" + 0.005*\"treatment\"'), (20, '0.023*\"vitamin\" + 0.022*\"deficiency\" + 0.021*\"anaemia\" + 0.020*\"splenic\" + 0.020*\"b12\" + 0.013*\"iron\" + 0.011*\"coeliac\" + 0.010*\"patient\" + 0.008*\"case\" + 0.005*\"diagnosis\"'), (137, '0.012*\"coronary\" + 0.012*\"myocardial\" + 0.012*\"patient\" + 0.011*\"cardiac\" + 0.010*\"ecg\" + 0.009*\"cardiomyopathy\" + 0.009*\"st\" + 0.009*\"acute\" + 0.008*\"normal\" + 0.007*\"patients\"'), (135, '0.166*\"pe\" + 0.024*\"ctpa\" + 0.024*\"dimer\" + 0.022*\"pulmonary\" + 0.021*\"pvt\" + 0.011*\"hpvg\" + 0.011*\"patient\" + 0.010*\"embolism\" + 0.009*\"thrombosis\" + 0.007*\"vein\"'), (131, '0.044*\"insulin\" + 0.022*\"diabetes\" + 0.016*\"glucose\" + 0.012*\"mmol\" + 0.011*\"mg\" + 0.011*\"patient\" + 0.010*\"diabetic\" + 0.008*\"case\" + 0.008*\"patients\" + 0.008*\"sle\"'), (132, '0.088*\"aneurysm\" + 0.059*\"aneurysms\" + 0.025*\"mycotic\" + 0.020*\"aep\" + 0.013*\"artery\" + 0.013*\"ruptured\" + 0.013*\"aneurysmal\" + 0.012*\"fish\" + 0.011*\"fusiform\" + 0.010*\"rupture\"'), (76, '0.054*\"nsaids\" + 0.047*\"ds\" + 0.037*\"cppd\" + 0.029*\"acromegaly\" + 0.026*\"gh\" + 0.020*\"crystal\" + 0.018*\"nsaid\" + 0.018*\"symphysis\" + 0.014*\"ace\" + 0.009*\"pubic\"'), (37, '0.019*\"gastric\" + 0.014*\"stomach\" + 0.010*\"ct\" + 0.010*\"volvulus\" + 0.010*\"obstruction\" + 0.009*\"bowel\" + 0.009*\"mesenteric\" + 0.009*\"abdominal\" + 0.007*\"case\" + 0.007*\"blood\"'), (50, '0.006*\"ai\" + 0.004*\"patient\" + 0.002*\"factor\" + 0.002*\"torsion\" + 0.002*\"case\" + 0.002*\"patients\" + 0.002*\"cases\" + 0.002*\"diagnosis\" + 0.001*\"treatment\" + 0.001*\"right\"'), (82, '0.025*\"adrenal\" + 0.011*\"hormone\" + 0.011*\"patient\" + 0.010*\"normal\" + 0.009*\"patients\" + 0.008*\"cortisol\" + 0.007*\"disease\" + 0.007*\"serum\" + 0.007*\"acth\" + 0.007*\"ml\"'), (55, '0.013*\"lesion\" + 0.010*\"tissue\" + 0.008*\"bone\" + 0.008*\"figure\" + 0.008*\"swelling\" + 0.008*\"case\" + 0.006*\"diagnosis\" + 0.006*\"examination\" + 0.006*\"right\" + 0.006*\"excision\"'), (61, '0.028*\"malt\" + 0.026*\"lymphoma\" + 0.017*\"cell\" + 0.017*\"fd\" + 0.012*\"lymphoid\" + 0.010*\"cells\" + 0.008*\"patient\" + 0.007*\"bccs\" + 0.007*\"diagnosis\" + 0.007*\"bcc\"'), (95, '0.017*\"statin\" + 0.015*\"pm\" + 0.015*\"myopathy\" + 0.014*\"muscle\" + 0.013*\"statins\" + 0.011*\"rhabdomyolysis\" + 0.010*\"occupational\" + 0.010*\"simvastatin\" + 0.009*\"exposure\" + 0.009*\"patients\"'), (88, '0.036*\"acupuncture\" + 0.019*\"bronchiectasis\" + 0.017*\"ball\" + 0.017*\"filaments\" + 0.014*\"nocardia\" + 0.012*\"tos\" + 0.010*\"patient\" + 0.009*\"dialysate\" + 0.009*\"pulmonary\" + 0.008*\"boerhaave\"'), (28, '0.050*\"breast\" + 0.015*\"cancer\" + 0.009*\"patient\" + 0.009*\"treatment\" + 0.008*\"case\" + 0.008*\"bleomycin\" + 0.007*\"axillary\" + 0.007*\"diagnosis\" + 0.005*\"may\" + 0.005*\"radiation\"'), (109, '0.049*\"duct\" + 0.023*\"bile\" + 0.017*\"calcification\" + 0.016*\"pancreatic\" + 0.015*\"pancreas\" + 0.011*\"cystic\" + 0.008*\"biliary\" + 0.008*\"ct\" + 0.008*\"figure\" + 0.007*\"common\"')]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 168 is out of bounds for axis 0 with size 150",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m doc_lda \u001b[38;5;241m=\u001b[39m model[corpus]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mprint_topics())\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topic_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m168\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mshow_topic(\u001b[38;5;241m6\u001b[39m, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/gensim/models/ldamodel.py:1248\u001b[0m, in \u001b[0;36mLdaModel.get_topic_terms\u001b[0;34m(self, topicid, topn)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_topic_terms\u001b[39m(\u001b[38;5;28mself\u001b[39m, topicid, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the representation for a single topic. Words the integer IDs, in constrast to\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m    :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1248\u001b[0m     topic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtopicid\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1249\u001b[0m     topic \u001b[38;5;241m=\u001b[39m topic \u001b[38;5;241m/\u001b[39m topic\u001b[38;5;241m.\u001b[39msum()  \u001b[38;5;66;03m# normalize to probability distribution\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     bestn \u001b[38;5;241m=\u001b[39m matutils\u001b[38;5;241m.\u001b[39margsort(topic, topn, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 168 is out of bounds for axis 0 with size 150"
     ]
    }
   ],
   "source": [
    "doc_lda = model[corpus]\n",
    "\n",
    "print(model.print_topics())\n",
    "model.get_topic_terms(168, topn=15)\n",
    "model.show_topic(6, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c189fa5-b757-4d2d-8339-b6ade6f1c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.top_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f178971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8491640",
   "metadata": {},
   "source": [
    "## SciKit LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fbdf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from time import time\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "\n",
    "# with open(FILE) as f:\n",
    "#     data_samples = f.readlines()\n",
    "\n",
    "#     print(data_samples)\n",
    "    \n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for LDA...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "# print(data_samples[0])\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "\n",
    "aux_lda = lda.fit(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in scikit LDA model\")\n",
    "\n",
    "print('LDA perplexity:', aux_lda.perplexity(tfidf[0]))\n",
    "# perplexity.append(news_lda.perplexity(doc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ea2d3",
   "metadata": {},
   "source": [
    "# ---------------------\n",
    "## Exemplo tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763744aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "#     print(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(topic)\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "#         print(top_features_ind)\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "#         print(top_features)\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "data, y = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "data_samples = data\n",
    "# data_samples = data[:n_samples]\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "# print(type(tfidf))\n",
    "# print(tfidf.shape)\n",
    "# print(len(tfidf[0]))\n",
    "\n",
    "# print(tfidf.toarray())\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bcd33",
   "metadata": {},
   "source": [
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "# This produces a feature matrix of token counts, similar to what CountVectorizer would produce on text.\n",
    "X, Y = make_multilabel_classification(random_state=0)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# get topics for some given samples:\n",
    "print(lda.transform(X[-2:]))\n",
    "print('Perplexity ', lda.perplexity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "texts = ['eu voce que tal', 'ola meu bem', 'bora?']\n",
    "fitted = vec.fit_transform(texts)\n",
    "fitted.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d376967",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "topics = [\n",
    "\n",
    "    ['human', 'computer', 'system', 'interface'],\n",
    "\n",
    "    ['graph', 'minors', 'trees', 'eps']\n",
    "\n",
    "]\n",
    "\n",
    "cm = CoherenceModel(topics=topics, corpus=common_corpus, dictionary=common_dictionary, coherence='u_mass')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print(coherence)\n",
    "print(cm.get_coherence_per_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "texts2 = [['eu voce que tal', 'ola meu bem', 'bora?']]\n",
    "\n",
    "\n",
    "dct = Dictionary(texts2)\n",
    "bow = dct.doc2bow([\"eu voce que tal\", \"ola meu bem\"])\n",
    "# bow = dct.doc2bow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33606cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct2 = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
    "str(dct2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
