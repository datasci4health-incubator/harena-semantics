{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88637ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "DATASET = 'clicr'\n",
    "\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "# FILE = '../datasets/' + DATASET + '/sequences.txt'\n",
    "DATASET_PATH = '../datasets/'+DATASET+'/titles-sem-mark.txt'\n",
    "\n",
    "OUTPUT_PATH = 'output/'\n",
    "\n",
    "with open(DATASET_PATH) as f:\n",
    "    data_samples = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8ccf6",
   "metadata": {},
   "source": [
    "## Gemsim LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07770730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary, common_texts, simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# def preprocess(sentences):\n",
    "#     for sentence in sentences:\n",
    "#         yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# print(data_samples[0])\n",
    "data_samples2 = data_samples\n",
    "\n",
    "# [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in data_samples2]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# data_samples2 = remove_stopwords(data_samples2)\n",
    "# print(data_samples2)\n",
    "# print(data_samples2)\n",
    "\n",
    "# lista = []\n",
    "# for sentence in data_samples2:\n",
    "# #     print()\n",
    "#     aqui = simple_preprocess(str(sentence), deacc=True)\n",
    "# #     print(aqui)\n",
    "\n",
    "\n",
    "# data_samples = preprocess(data_samples2)\n",
    "\n",
    "# print('lista, :', lista)\n",
    "\n",
    "for idx in range(len(data_samples2)):\n",
    "#     print(data_samples2[idx])\n",
    "    data_samples2[idx] = data_samples2[idx].lower()  # Convert to lowercase.\n",
    "    data_samples2[idx] = tokenizer.tokenize(data_samples2[idx])  # Split into words.\n",
    "\n",
    "# print(data_samples2[0])\n",
    "\n",
    "# # Remove numbers, but not words that contain numbers.\n",
    "data_samples2 = [[token for token in doc if not token.isnumeric()] for doc in data_samples2]\n",
    "data_samples2 = [[token for token in doc if token not in stop_words] for doc in data_samples2]\n",
    "\n",
    "# # Remove words that are only one character.\n",
    "data_samples2 = [[token for token in doc if len(token) > 1] for doc in data_samples2]\n",
    "\n",
    "# print(data_samples2)\n",
    "\n",
    "\n",
    "dictionary = Dictionary(data_samples2)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_samples2]\n",
    "texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n",
    "\n",
    "# print(dictionary)\n",
    "# print(corpus)\n",
    "# print(data_samples2[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ade6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dictionary)\n",
    "N_TOPICS = 200\n",
    "model = LdaModel(corpus, N_TOPICS, dictionary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1405b85-e141-4895-9b24-7c6c25fd20dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coerência: 0.5401877624866986\n",
      "Coerência por Tópico:  [0.529084138452087, 0.529084138452087, 0.5066192678287513, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.5604341174336351, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6307421187760076, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.4983140703917659, 0.529084138452087, 0.19596418464752613, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.8854488680690207, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6690429568963008, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.601911533008934, 0.7343544692285965, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.8381798040625836, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.22247927393533762, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6092677287412542, 0.529084138452087, 0.529084138452087, 0.18868727784356376, 0.7085026689546675, 0.5250858312555887, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6063921438329001, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.8073542388289854, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6573853150701325, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6626679374661146, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.4919693587933942, 0.529084138452087, 0.529084138452087, 0.7528526335829133, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6552707889857401, 0.529084138452087, 0.5350863785871269, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6825974978156963, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.710469675292033, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.7744393895793763, 0.529084138452087, 0.5115328334213205, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.6597729901319648, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.529084138452087, 0.5482646385833071, 0.529084138452087, 0.6621589694403809]\n"
     ]
    }
   ],
   "source": [
    "TOP_WORDS = 20\n",
    "cm = CoherenceModel(model=model, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v', topn=TOP_WORDS)\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "\n",
    "# print(str(doc_lda))\n",
    "\n",
    "print('Coerência:',coherence)\n",
    "print('Coerência por Tópico: ',coherence_per_topic)\n",
    "\n",
    "with open(OUTPUT_PATH + 'ldaresults.txt', \"a\") as file:\n",
    "    # print('Limiar de distância: '+str(distance_threshold), file=file)   \n",
    "    print('Qtdd de Tópicos: '+str(N_TOPICS), file=file)   \n",
    "    # print('Limiar TF-IDF: '+str(MAX_DF), file=file)  \n",
    "    print('Coerência total: '+str(coherence), file=file)  \n",
    "    print('Coerência por Tópicos: '+str(coherence_per_topic), file=file)  \n",
    "    print('Top Words: '+str(TOP_WORDS), file=file)  \n",
    "\n",
    "    print('----------------------------------------------------------------------------', file=file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56954571-a978-45f2-9f17-e21c1849cc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(27, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (33, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (79, '0.176*\"vein\" + 0.113*\"following\" + 0.106*\"trauma\" + 0.097*\"thrombosis\" + 0.074*\"low\" + 0.072*\"lower\" + 0.055*\"blunt\" + 0.053*\"deep\" + 0.047*\"syndrome\" + 0.040*\"limb\"'), (136, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (100, '0.346*\"following\" + 0.216*\"pain\" + 0.145*\"fracture\" + 0.085*\"internal\" + 0.045*\"foot\" + 0.034*\"fixation\" + 0.019*\"open\" + 0.015*\"reduction\" + 0.010*\"arthroscopic\" + 0.007*\"lateral\"'), (51, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (107, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (169, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (7, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (28, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (199, '0.314*\"artery\" + 0.175*\"coronary\" + 0.159*\"left\" + 0.099*\"ct\" + 0.033*\"anomalous\" + 0.028*\"origin\" + 0.024*\"ischaemia\" + 0.021*\"silent\" + 0.020*\"death\" + 0.019*\"risk\"'), (187, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (109, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (38, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (17, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (127, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (192, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (21, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (174, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"'), (99, '0.000*\"neuronopathy\" + 0.000*\"focus\" + 0.000*\"isoniazid\" + 0.000*\"hypocomplementaemic\" + 0.000*\"urticarial\" + 0.000*\"neuroacanthocytosis\" + 0.000*\"nothing\" + 0.000*\"seems\" + 0.000*\"fnac\" + 0.000*\"indicators\"')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('neuronopathy', 8.381527e-05),\n",
       " ('focus', 8.381527e-05),\n",
       " ('isoniazid', 8.381527e-05),\n",
       " ('hypocomplementaemic', 8.381527e-05),\n",
       " ('urticarial', 8.381527e-05),\n",
       " ('neuroacanthocytosis', 8.381527e-05),\n",
       " ('nothing', 8.381527e-05),\n",
       " ('seems', 8.381527e-05),\n",
       " ('fnac', 8.381527e-05),\n",
       " ('indicators', 8.381527e-05),\n",
       " ('microembolism', 8.381527e-05),\n",
       " ('page', 8.381527e-05),\n",
       " ('hurthle', 8.381527e-05),\n",
       " ('healer', 8.381527e-05),\n",
       " ('jung', 8.381527e-05)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lda = model[corpus]\n",
    "\n",
    "print(model.print_topics())\n",
    "model.get_topic_terms(168, topn=15)\n",
    "model.show_topic(6, topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f178971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8491640",
   "metadata": {},
   "source": [
    "## SciKit LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69fbdf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for LDA...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(data_samples[0])\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone in \u001b[39m\u001b[38;5;132;01m%0.3f\u001b[39;00m\u001b[38;5;124ms.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (time() \u001b[38;5;241m-\u001b[39m t0))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Use tf (raw term count) features for LDA.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2077\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m \n\u001b[1;32m   2060\u001b[0m \u001b[38;5;124;03mThis is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;124;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m-> 2077\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2079\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1327\u001b[0m             )\n\u001b[1;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from time import time\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "# DATASET = 'OSCE/dor_toracica_x_infarto'\n",
    "# FILE = '../datasets/' + DATASET + '/respostas_ingles.txt'\n",
    "\n",
    "# with open(FILE) as f:\n",
    "#     data_samples = f.readlines()\n",
    "\n",
    "#     print(data_samples)\n",
    "    \n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for LDA...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "# print(data_samples[0])\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "\n",
    "aux_lda = lda.fit(tfidf)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in scikit LDA model\")\n",
    "\n",
    "print('LDA perplexity:', aux_lda.perplexity(tfidf[0]))\n",
    "# perplexity.append(news_lda.perplexity(doc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ea2d3",
   "metadata": {},
   "source": [
    "# ---------------------\n",
    "## Exemplo tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763744aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 7\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "#     print(model.components_)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "#         print(topic)\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "#         print(top_features_ind)\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "#         print(top_features)\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "data, y = fetch_20newsgroups(\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True,\n",
    ")\n",
    "data_samples = data\n",
    "# data_samples = data[:n_samples]\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "# print(type(tfidf))\n",
    "# print(tfidf.shape)\n",
    "# print(len(tfidf[0]))\n",
    "\n",
    "# print(tfidf.toarray())\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bcd33",
   "metadata": {},
   "source": [
    "# ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "# This produces a feature matrix of token counts, similar to what CountVectorizer would produce on text.\n",
    "X, Y = make_multilabel_classification(random_state=0)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda.fit(X)\n",
    "\n",
    "# get topics for some given samples:\n",
    "print(lda.transform(X[-2:]))\n",
    "print('Perplexity ', lda.perplexity(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "\n",
    "texts = ['eu voce que tal', 'ola meu bem', 'bora?']\n",
    "fitted = vec.fit_transform(texts)\n",
    "fitted.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d376967",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f8be30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "topics = [\n",
    "\n",
    "    ['human', 'computer', 'system', 'interface'],\n",
    "\n",
    "    ['graph', 'minors', 'trees', 'eps']\n",
    "\n",
    "]\n",
    "\n",
    "cm = CoherenceModel(topics=topics, corpus=common_corpus, dictionary=common_dictionary, coherence='u_mass')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print(coherence)\n",
    "print(cm.get_coherence_per_topic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "texts2 = [['eu voce que tal', 'ola meu bem', 'bora?']]\n",
    "\n",
    "\n",
    "dct = Dictionary(texts2)\n",
    "bow = dct.doc2bow([\"eu voce que tal\", \"ola meu bem\"])\n",
    "# bow = dct.doc2bow([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33606cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct2 = Dictionary([\"máma mele maso\".split(), \"ema má máma\".split()])\n",
    "str(dct2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
