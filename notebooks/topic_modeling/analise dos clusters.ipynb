{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "# MODEL = 'NLI'\n",
    "# MODEL = 'nli_clinical_bert'\n",
    "MODEL = 'fagner/envoy'\n",
    "\n",
    "# DATASET = 'control_test'\n",
    "# DATASET = 'exame_OSCE_cefaleia'\n",
    "DATASET = 'OSCE'\n",
    "\n",
    "TSV_PATH = '../from_embeddings_to_tsv/output3/' + DATASET + '/' + MODEL + '/'\n",
    "# TSV_PATH = '../sentence_embedding/tsv_files/' + DATASET + '/' + MODEL + '/'\n",
    "# TSV_PATH = '../sentence_embedding/output/' + DATASET + '/' + MODEL + '/'\n",
    "\n",
    "IMG_PATH = 'imgs/'\n",
    "\n",
    "if not os.path.exists(IMG_PATH):\n",
    "    os.mkdir(IMG_PATH)\n",
    "\n",
    "IMG_PATH +=  DATASET + '/'\n",
    "\n",
    "if not os.path.exists(IMG_PATH):\n",
    "    os.mkdir(IMG_PATH)\n",
    "    \n",
    "IMG_PATH += MODEL + '/'\n",
    "\n",
    "# os.makedirs(path, exist_ok=True)\n",
    "if not os.path.exists(IMG_PATH):\n",
    "    os.makedirs(IMG_PATH, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(TSV_PATH):\n",
    "    os.makedirs(TSV_PATH, exist_ok=True)\n",
    "#     os.mkdir(TSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "embeddings_df = pd.read_csv(TSV_PATH + 'word_embeddings.tsv', sep='\\t',header=None)\n",
    "# embeddings_df = pd.read_csv('../sentence_embedding/tsv_files/stsb-roberta-large/exame-OSCE/cefaleia/word_embeddings.tsv', sep='\\t',header=None)\n",
    "\n",
    "sentences_df = pd.read_csv(TSV_PATH + 'labels.tsv', sep='\\t',header=None)\n",
    "# sentences_df = pd.read_csv('../sentence_embedding/tsv_files/paraphrase-distilroberta-base-v1/exame-OSCE/cefaleia/labels.tsv', sep='\\t',header=None)\n",
    "\n",
    "print(sentences_df)\n",
    "embeddings_numpy = embeddings_df.to_numpy()\n",
    "sentences_numpy = sentences_df.to_numpy()\n",
    "\n",
    "X_embeddings = embeddings_numpy[:, :-1]\n",
    "\n",
    "provas = []\n",
    "for i in range(len(X_embeddings)):\n",
    "    provas.append(i)\n",
    "# embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_clustering(X, labels, title=None):\n",
    "    x_min, x_max = np.min(X, axis=0), np.max(X, axis=0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(provas[i])+'.', color=plt.cm.nipy_spectral(labels[i] / 10.),\n",
    "                 fontdict={'weight': 'bold'})\n",
    "        plt.scatter(X[i, 0], X[i, 1], color=plt.cm.nipy_spectral(labels[i] / 10.), s=40, cmap='hsv_r')\n",
    "\n",
    "# Define os valores nos eixos\n",
    "    plt.xticks([0,1.1])\n",
    "    plt.yticks([0,1.1])\n",
    "    plt.title(title, size=17)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    plt.savefig(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustering_v2(X, labels, title=None):\n",
    "    X_2d\n",
    "    clustered_x = []\n",
    "    clustered_y = []\n",
    "    for x in X:\n",
    "        clustered_x.append(x[0])\n",
    "        clustered_y.append(x[1])\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    for i in range(len(clustered_x)):\n",
    "        plt.text(clustered_x[i], clustered_y[i], provas[i], color=plt.cm.nipy_spectral(labels[i] / 10.),\n",
    "                         fontdict={'weight': 'bold'})\n",
    "\n",
    "    plt.scatter(clustered_x, clustered_y, color=plt.cm.nipy_spectral(agglomerative.labels_ / 10.), s=40, cmap='hsv_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn import manifold\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "X_2d = manifold.SpectralEmbedding(n_components=2).fit_transform(X_embeddings)\n",
    "\n",
    "x = X_2d[:,0]\n",
    "y = X_2d[:,1]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 10))\n",
    "\n",
    "set_of_colors = []\n",
    "n_clusters = []\n",
    "distance_thresholds = []\n",
    "models = []\n",
    "print('s')\n",
    "\n",
    "distance_thresholds = [7, 8, 9, 10, 16, 20, 25]\n",
    "# distance_thresholds = [5, 10, 15, 20, 22, 24, 25]\n",
    "# distance_thresholds = [0.00025, 0.00017, 0.00013, 0.00009, 0.00007, 0.00006, 0.00005]\n",
    "# distance_thresholds = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "# distance_thresholds = [7, 13, 18, 22, 28, 37, 43]\n",
    "# distance_thresholds = [1, 5, 7, 9, 10, 12, 15, ]\n",
    "\n",
    "for i in distance_thresholds:\n",
    "    print(i)\n",
    "#     distance_thresholds.append(11+i)\n",
    "    agglomerative = AgglomerativeClustering(linkage='ward', distance_threshold=i, n_clusters=None)\n",
    "\n",
    "    agglomerative = agglomerative.fit(X_embeddings)\n",
    "    \n",
    "    models.append(agglomerative)\n",
    "    \n",
    "    n_clusters.append(agglomerative.n_clusters_)\n",
    "    colors = []\n",
    "    for l in agglomerative.labels_:\n",
    "        colors.append(plt.cm.nipy_spectral(l / 10.))\n",
    "    set_of_colors.append(colors)\n",
    "    \n",
    "ax[0][0].scatter(x, y, color=set_of_colors[0], s=40, cmap='hsv_r')\n",
    "ax[0][0].title.set_text(str(n_clusters[0]) + ' grupos | Limiar=' + str(distance_thresholds[0]))\n",
    "\n",
    "# ax[0][1].scatter(x, y, color=set_of_colors[1], s=40, cmap='hsv_r')\n",
    "# ax[0][1].title.set_text(str(n_clusters[1]) + ' grupos | Limiar=' + str(distance_thresholds[1]))\n",
    "\n",
    "# ax[0][2].scatter(x, y, color=set_of_colors[2], s=40, cmap='hsv_r')\n",
    "# ax[0][2].title.set_text(str(n_clusters[2]) + ' grupos | Limiar=' + str(distance_thresholds[2]))\n",
    "\n",
    "# ax[1][0].scatter(x, y, color=set_of_colors[3], s=40, cmap='hsv_r')\n",
    "# ax[1][0].title.set_text(str(n_clusters[3]) + ' grupos | Limiar=' + str(distance_thresholds[3]))\n",
    "\n",
    "# ax[1][1].scatter(x, y, color=set_of_colors[4], s=40, cmap='hsv_r')\n",
    "# ax[1][1].title.set_text(str(n_clusters[4]) + ' grupos | Limiar=' + str(distance_thresholds[4]))\n",
    "\n",
    "# ax[1][2].scatter(x, y, color=set_of_colors[5], s=40, cmap='hsv_r')\n",
    "# ax[1][2].title.set_text(str(n_clusters[5]) + ' grupos | Limiar=' + str(distance_thresholds[5]))\n",
    "\n",
    "# ax[2][0].scatter(x, y, color=set_of_colors[6], s=40, cmap='hsv_r')\n",
    "# ax[2][0].title.set_text(str(n_clusters[6]) + ' grupos | Limiar=' + str(distance_thresholds[6]))\n",
    "\n",
    "plt.savefig(IMG_PATH + 'clusters')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=3, ncols=3, figsize=(17, 15))\n",
    "\n",
    "# Sem aplicar truncamento no dendograma\n",
    "ax[0][0].title.set_text('Sem truncamento')\n",
    "plot_dendrogram(models[0], truncate_mode=None, ax=ax[0][0])\n",
    "\n",
    "# # Truncando por nível\n",
    "# # Não mais do que p níveis da árvore do dendrograma são exibidos. Um “nível” inclui todos os nós com mesclagens p da última mesclagem. \n",
    "# ax[0][1].title.set_text('Truncamento por nível = '+ str(12))\n",
    "# plot_dendrogram(models[1], truncate_mode='level', p=12, ax=ax[0][1])\n",
    "\n",
    "# ax[0][2].title.set_text('Truncamento por nível = '+ str(7))\n",
    "# plot_dendrogram(models[2], truncate_mode='level', p=7, ax=ax[0][2])\n",
    "\n",
    "# ax[1][0].title.set_text('Truncamento por nível = '+ str(5))\n",
    "# plot_dendrogram(models[3], truncate_mode='level', p=5, ax=ax[1][0])\n",
    "# ax[1][1].title.set_text('Truncamento por nível = '+ str(4))\n",
    "# plot_dendrogram(models[4], truncate_mode='level', p=4, ax=ax[1][1])\n",
    "# ax[1][2].title.set_text('Truncamento por nível = '+ str(3))\n",
    "# plot_dendrogram(models[5], truncate_mode='level', p=3, ax=ax[1][2])\n",
    "# ax[2][0].title.set_text('Truncamento por nível = '+ str(1))\n",
    "# plot_dendrogram(models[6], truncate_mode='level', p=1, ax=ax[2][0])\n",
    "\n",
    "\n",
    "# # Os últimos p clusters não únicos, formados na rede são os únicos nós não folha na rede; eles correspondem às linhas Z [n-p-2: fim] em Z. Todos os outros clusters não singleton são contraídos em nós folha. \n",
    "# ax[2][1].title.set_text('Truncamento por número de clusters = '+ str(113))\n",
    "# plot_dendrogram(models[6], truncate_mode='lastp', p=113, ax=ax[2][1])\n",
    "# ax[2][2].title.set_text('Truncamento por número de clusters = '+ str(2))\n",
    "# plot_dendrogram(models[6], truncate_mode='lastp', p=2, ax=ax[2][2])\n",
    "\n",
    "plt.savefig(IMG_PATH + 'dendograms')\n",
    "# plot_dendrogram(models[2], truncate_mode='lastp', p=6, ax=ax[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "# tf_idf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.1)\n",
    "\n",
    "print(n_clusters)\n",
    "print()\n",
    "\n",
    "k = 0\n",
    "for n in n_clusters:\n",
    "    print(models[k])\n",
    "    print('n clusters: ',n)\n",
    "\n",
    "    clusters_of_sentences = [''] * n\n",
    "\n",
    "    for i in range(len(models[k].labels_)):\n",
    "        sentence = ''\n",
    "        sentence = sentences_numpy[i] + ' | '\n",
    "        clusters_of_sentences[models[k].labels_[i]] += sentence\n",
    "    \n",
    "    clusters_of_sentences_formato_tfidf = []\n",
    "    for d_c in clusters_of_sentences:\n",
    "        clusters_of_sentences_formato_tfidf.append(str(d_c).split('[')[1].replace('[', '').replace(']\\n', '').replace('\\'', ''))\n",
    "\n",
    "    tfidf = tf_idf_vectorizer.fit_transform(clusters_of_sentences_formato_tfidf)\n",
    "    feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "\n",
    "    tfidf_matrix = tfidf.toarray()\n",
    "\n",
    "    i=0\n",
    "    topics_by_cluster = []\n",
    "    wordclouds = []\n",
    "    for d_c in clusters_of_sentences_formato_tfidf:\n",
    "#         print(d_c)\n",
    "        print('Cluster: ', i)\n",
    "\n",
    "        sorted_index_array = np.argsort(tfidf_matrix[i]) \n",
    "        highests = sorted_index_array[-10:]\n",
    "\n",
    "        topics = []\n",
    "        print('Topics (ascending order):')\n",
    "        for s in highests:\n",
    "            print(feature_names[s])\n",
    "            topics.append(feature_names[s])\n",
    "\n",
    "        topics_by_cluster.append(topics)\n",
    "\n",
    "        dense = tfidf[i].todense()\n",
    "        lst1 = dense.tolist()\n",
    "        df = pd.DataFrame(lst1, columns=feature_names)\n",
    "\n",
    "        wordcloud = WordCloud(background_color=\"white\", max_words=50).generate_from_frequencies(df.T.sum(axis=1))\n",
    "        wordclouds.append(wordcloud)\n",
    "\n",
    "        print()\n",
    "        i+=1\n",
    "    fig, axs = plt.subplots(n, figsize=(10, 20))\n",
    "        \n",
    "    j=0\n",
    "\n",
    "    for wc in wordclouds:\n",
    "        axs[j].title.set_text('Cluster '+str(j))\n",
    "        axs[j].imshow(wc)\n",
    "        axs[j].axis(\"off\")\n",
    "        j+=1\n",
    "        \n",
    "#     print(topics_by_cluster)\n",
    "\n",
    "    k+=1\n",
    "    plt.savefig(IMG_PATH + 'topicos_por_'+str(n)+'_clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
