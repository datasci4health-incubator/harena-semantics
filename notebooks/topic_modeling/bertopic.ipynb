{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef7dc4c-44d5-401e-b627-a03b8a5fff0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in /opt/conda/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /opt/conda/lib/python3.9/site-packages (from bertopic) (4.64.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.9/site-packages (from bertopic) (1.4.2)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /opt/conda/lib/python3.9/site-packages (from bertopic) (0.5.3)\n",
      "Requirement already satisfied: pyyaml<6.0 in /opt/conda/lib/python3.9/site-packages (from bertopic) (5.4.1)\n",
      "Requirement already satisfied: hdbscan>=0.8.28 in /opt/conda/lib/python3.9/site-packages (from bertopic) (0.8.28)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /opt/conda/lib/python3.9/site-packages (from bertopic) (5.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.9/site-packages (from bertopic) (1.21.6)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/lib/python3.9/site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from bertopic) (2.2.0)\n",
      "Requirement already satisfied: cython>=0.27 in /opt/conda/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.12.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.18.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.11.0)\n",
      "Requirement already satisfied: numba>=0.49 in /opt/conda/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.7)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (62.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.0.49)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.3.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "! pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34475cf-9370-4d8a-a08f-9b5dd49549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    " \n",
    "DATASET = 'clicr'\n",
    "DATASET_PATH = '../datasets/'+DATASET+'/clinicalcases.tsv'\n",
    "\n",
    "docs = []\n",
    "with open(DATASET_PATH) as f:\n",
    "    docs = f.readlines()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0777d63-ee25-4c53-9f73-c3cd556a56e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eye', 0.020788212236196223),\n",
       " ('retinal', 0.01158268758095172),\n",
       " ('visual', 0.011464713156527811),\n",
       " ('corneal', 0.010752290766083233),\n",
       " ('ocular', 0.00904652307784903),\n",
       " ('vision', 0.008306469530144859),\n",
       " ('acuity', 0.008177445332635492),\n",
       " ('optic', 0.0074606626844610216),\n",
       " ('eyes', 0.007082028357956753),\n",
       " ('intraocular', 0.006357338503787615),\n",
       " ('lens', 0.006281248971165769),\n",
       " ('macular', 0.006084666379091111),\n",
       " ('in', 0.005624864044002651),\n",
       " ('the', 0.0055048389676119166),\n",
       " ('anterior', 0.0054194708502483075),\n",
       " ('topical', 0.005366657925933672),\n",
       " ('oct', 0.0050981835376480615),\n",
       " ('fundus', 0.005002267262298498),\n",
       " ('right', 0.004992430022931915),\n",
       " ('20', 0.004952654725584291),\n",
       " ('vitreous', 0.004942078271063204),\n",
       " ('of', 0.004934567106443933),\n",
       " ('iop', 0.004932660512533555),\n",
       " ('was', 0.00485530730587198),\n",
       " ('with', 0.004834874665314684),\n",
       " ('to', 0.004769546523738599),\n",
       " ('and', 0.004686271713909139),\n",
       " ('left', 0.004592636251608792),\n",
       " ('glaucoma', 0.00455730249622138),\n",
       " ('choroidal', 0.00452216283393138)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model = BERTopic(top_n_words=30)\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "topic_model.get_topics()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90e0b6fa-1e32-4b75-9a65-ec4415021549",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_by_topic = []\n",
    "for topic in topic_model.get_topics().values():\n",
    "    terms = []\n",
    "    for item in topic:\n",
    "        terms.append(item[0])\n",
    "    terms_by_topic.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "607a9fc7-b131-4868-b19f-199f3eaa8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "MAX_DF = 0.95\n",
    "\n",
    "# for i in range(len(sentences)):\n",
    "#     topic_index = clustering_model.labels_[i]\n",
    "#     documents_by_topic[topic_index].append(sentences[i])\n",
    "#     topics[topic_index] += sentences[i] + \" \"\n",
    "\n",
    "tfidf_model = TfidfVectorizer(max_df=MAX_DF)\n",
    "\n",
    "document_term_matrix = tfidf_model.fit_transform(docs)\n",
    "terms = tfidf_model.get_feature_names_out()\n",
    "terms_by_topic = tfidf_model.inverse_transform(document_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e86b606-ee41-4597-ac47-3466064c1b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97655"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_by_topic = tfidf_model.inverse_transform(document_term_matrix)\n",
    "\n",
    "print(len(terms_by_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0869d2f3-d921-44fd-be82-fb93fdd83943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "data_samples2 = docs\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for idx in range(len(data_samples2)):\n",
    "    data_samples2[idx] = data_samples2[idx].lower()  # Convert to lowercase.\n",
    "    data_samples2[idx] = tokenizer.tokenize(data_samples2[idx])  # Split into words.\n",
    "\n",
    "# # Remove numbers, but not words that contain numbers.\n",
    "data_samples2 = [[token for token in doc if not token.isnumeric()] for doc in data_samples2]\n",
    "data_samples2 = [[token for token in doc if token not in stop_words] for doc in data_samples2]\n",
    "\n",
    "# # Remove words that are only one character.\n",
    "data_samples2 = [[token for token in doc if len(token) > 1] for doc in data_samples2]\n",
    "\n",
    "dictionary = Dictionary(data_samples2)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_samples2]\n",
    "texts = [[dictionary[word_id] for word_id, freq in doc] for doc in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02edc1-f36c-4efc-b9fe-f6d5da7214e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "TOP_WORDS = 30\n",
    "\n",
    "dictionary = Dictionary(terms_by_topic)\n",
    "corpus = [dictionary.doc2bow(t) for t in terms_by_topic]\n",
    "texts = [[dictionary[word_id] for word_id, freq in c] for c in corpus]\n",
    "\n",
    "cm = CoherenceModel(topics=terms_by_topic, texts=texts, corpus=corpus, dictionary=dictionary, coherence='c_v',topn=TOP_WORDS)\n",
    "\n",
    "coherence = cm.get_coherence()\n",
    "coherence_per_topic = cm.get_coherence_per_topic()\n",
    "\n",
    "print('Coerencia: ', coherence)\n",
    "print('Coerencia por Topico: ', coherence_per_topic)    \n",
    "\n",
    "with open(OUTPUT_PATH + '/mntresults.txt', \"a\") as file:\n",
    "    print('Limiar de distância: '+str(DISTANCE_THRESHOLD), file=file)   \n",
    "    print('Qtdd de Tópicos: '+str(n_clusters), file=file)   \n",
    "    print('Limiar TF-IDF: '+str(MAX_DF), file=file)  \n",
    "    print('Coerência total: '+str(coherence), file=file)  \n",
    "    print('Coerência por Tópicos: '+str(coherence_per_topic), file=file)  \n",
    "    print('Top Words: '+str(TOP_WORDS), file=file)  \n",
    "\n",
    "    print('----------------------------------------------------------------------------', file=file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ad708-1795-4b57-a0df-359e3a5a8172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
